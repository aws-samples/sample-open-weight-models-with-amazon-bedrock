{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "workshop-title",
   "metadata": {},
   "source": [
    "# re:Invent 2025 - AIM311: Optimize Open Weight Models for Low-Latency, Cost-Effective AI Apps\n",
    "\n",
    "## Lab 1a: Model Selection Framework\n",
    "**Focus**: Use-case-driven model selection, pricing analysis, performance optimization\n",
    "\n",
    "### What You'll Learn in Lab 1a\n",
    "\n",
    "This lab takes a **use-case-first approach** to model selection. Instead of starting with technical metrics, we'll help you:\n",
    "\n",
    "1. **Identify your use case** - Understand whether you're building a chatbot, agent, document analyzer, or multimodal application\n",
    "2. **Select the right model** - Learn which models match your specific requirements (latency, context window, capabilities)\n",
    "3. **Validate with benchmarks** - Review independent performance data to inform your decision\n",
    "4. **Analyze costs and performance** - Test real pricing and latency metrics with hands-on examples\n",
    "\n",
    "### Models We'll Explore\n",
    "\n",
    "This workshop covers 7 open-weight models available on AWS Bedrock:\n",
    "\n",
    "- **Llama 4 Maverick** (400B MoE) - Multimodal chat and analysis with 1M context\n",
    "- **Llama 4 Scout** (109B MoE) - Ultra-long document processing with 3.5M context\n",
    "- **GPT OSS 120B** - Complex reasoning and agentic workflows\n",
    "- **GPT OSS 20B** - Fast, cost-effective responses for high-volume applications\n",
    "- **Qwen3 235B MoE** - Advanced reasoning with thinking mode\n",
    "- **Qwen3 32B** - Balanced performance for enterprise applications\n",
    "- **DeepSeek V3.1** (685B MoE) - Cost-optimized for high-volume deployments\n",
    "\n",
    "**Bonus**: We'll also explore **Qwen3 Coder** models (480B MoE, 30B MoE) specialized for code generation.\n",
    "\n",
    "**What is a Mixture of Experts (MoE)?**: Mixture of Experts (MoE) is an LLM architecture that uses multiple specialized \"expert\" networks within each layer, but only activates a subset of them for each input token. This sparse activation enables models to scale to several billion parameters while maintaining reasonable inference speed. However, MoE comes with higher memory requirements since all experts must be loaded.\n",
    "\n",
    "\n",
    "### Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Environment Setup\n",
    "**‚è±Ô∏è Pre-workshop setup - run these cells before starting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-dependencies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: asyncio is built-in to Python 3, no need to install\n",
    "! pip install -q rich pandas Pillow\n",
    "! pip install -q boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from rich.console import Console\n",
    "\n",
    "# Initialize console for rich output\n",
    "console = Console()\n",
    "\n",
    "# AWS Configuration\n",
    "AWS_REGION = \"us-west-2\"\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=AWS_REGION)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-case-section-header",
   "metadata": {},
   "source": [
    "## üéØ Step 1: Identify Your Use Case\n",
    "\n",
    "Before diving into pricing and performance metrics, let's start with the most important question: **What are you building?**\n",
    "\n",
    "Different use cases have different requirements. A real-time chatbot needs low latency, while a document analysis system needs a large context window. \n",
    "\n",
    "By identifying your use case first, you can narrow down which models are relevant to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-case-requirements-mapping",
   "metadata": {},
   "source": [
    "### Common Use Case Categories and Requirements\n",
    "\n",
    "Understanding the relationship between your use case and model capabilities helps you make informed decisions:\n",
    "\n",
    "\n",
    "| Use Case | Example Applications | Key Requirements | Priorities | Model Constraints |\n",
    "|----------|---------------------|------------------|---------------|-------------------|\n",
    "| Chatbot (text) | Customer support<br>Q&A systems<br>Virtual assistants | Low latency (<500ms TTFT)<br>Streaming support<br>Conversational memory | Speed > Cost> Quality | Fast inference<br>Streaming support<br>Cost at scale |\n",
    "| Agent | Research assistants<br>Automation workflows<br>Data analysis | Tool calling<br>Multi-step reasoning<br>Function execution | Quality > Tool Calling > Speed | Complex logic and strong reasoning<br>High reliability<br>Error handling |\n",
    "| Long Document Analysis | Legal review<br>Medical records<br>Contract analysis | Processing long documents<br>High accuracy | Context > Quality > Cost | Large context window<br>multimodal architecture<br> Extraction precision<br>Strong comprehension |\n",
    "| Code Generation | IDE assistants<br>Code review,<br>Documentation generation | Code understanding<br>Multi-language support<br>Syntax accuracy | Code Quality > Context > Speed | Trained on multiple programming languages<br>Context awareness<br>Understanding of programming patterns |\n",
    "| Multimodal Case | Document OCR<br>Visual Q&A<br>Image description | Vision + text processing<br>Image understanding | Multimodal Support > Quality > Speed | High quality image encoding<br>Multiple format support<br>Visual reasoning |\n",
    "\n",
    "\n",
    "**üí° Pro Tip**: Most real-world applications combine multiple use cases. For example, a customer support system might need both chatbot capabilities (low latency) and document analysis (knowledge base search). In these cases, prioritize your primary use case and validate that secondary requirements are met.\n",
    "\n",
    "**üí≠ Think about:** Based on the use cases you just learned about, which capability do you think is MOST important for your application? Consider multimodal support, large context windows, thinking mode, cost, or response time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-data-table",
   "metadata": {},
   "source": [
    "## üìà Step 2: High Level Model Comparison and Use Case Mapping\n",
    "\n",
    "The table below combines key performance metrics with use case recommendations to help you narrowing down the model choice for your application.\n",
    "\n",
    "| Model | Context Window | Best Use Case Match | Potential Applications |\n",
    "|-------|----------------|---------------------|----------------------|\n",
    "| DeepSeek V3.1 | 128K | Complex reasoning & agents | **Primary**: Agent systems<br>**Alternative:** Code generation (thinking mode)<br>**Budget:** Chatbots (non-thinking mode) |\n",
    "| GPT OSS 120B | 128K | Complex reasoning & agents | **Primary**: Agent systems<br>**Alternative:** Complex reasoning tasks |\n",
    "| GPT OSS 20B | 128K | Fast, cost-effective responses | **Primary**: Chatbots<br>**Alternative:** High-volume applications |\n",
    "| Qwen3 235B MoE | 128K | Reasoning with thinking mode | **Primary**: Agent systems<br>**Alternative:** Complex reasoning |\n",
    "| Qwen3 32B | 128K | Balanced enterprise apps | **Primary**: Chatbots<br>**Alternative:** General enterprise use |\n",
    "| Qwen3 Coder 480B | 256K | Code generation & analysis | **Primary**: Code generation<br>**Alternative:** Complex coding tasks |\n",
    "| Qwen3 Coder 30B | 256K | Code generation & analysis | **Primary**: Budget code generation<br>**Alternative:** Simple coding tasks |\n",
    "| Llama 4 Maverick | 1M | Multimodal applications | **Primary**: Complex OCR & charts<br>**Alternative:** Document analysis |\n",
    "| Llama 4 Scout | 3.5M | Ultra-long document analysis | **Primary**: Long documents<br>**Alternative:** Cost-effective multimodal |\n",
    "\n",
    "#### ‚ö†Ô∏è Important Notes\n",
    "\n",
    "| Category | Details |\n",
    "|----------|---------|\n",
    "| Regional Availability | ‚Ä¢ New models are regularly added to AWS Bedrock <br>‚Ä¢ Not all models are available in all AWS regions<br>‚Ä¢ Pricing may vary by region<br>‚Ä¢ Check the [AWS Bedrock documentation](https://docs.aws.amazon.com/bedrock/) for current availability |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-interpretation-guide",
   "metadata": {},
   "source": [
    "### üéØ How to Read Benchmarks for Your Use Case\n",
    "\n",
    "Different use cases prioritize different metrics. You need to identify what matters most for your application.\n",
    "\n",
    "Typical operational metrics:\n",
    "- **TTFT (Time to First Token)**: How quickly the model starts responding - critical for real-time applications\n",
    "- **Throughput**: Tokens generated per second - affects streaming speed and user experience\n",
    "- **Cost**: Price per million tokens - important for high-volume deployments\n",
    "- **Context Need**: Maximum input size required for your use case\n",
    "\n",
    "General Model Quality Considerations:\n",
    "- Larger models typically provide better reasoning and accuracy but at higher cost and latency\n",
    "- Specialized models (like code-focused variants) excel in their domain but may underperform in general tasks\n",
    "- MoE (Mixture of Experts) models offer good performance-to-cost ratios for diverse workloads\n",
    "- \"Thinking mode\" capabilities enhance complex reasoning but increase token usage and response time\n",
    "- Consider the trade-off between model capability and operational requirements for your specific use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-resources-links",
   "metadata": {},
   "source": [
    "### üîó External Resources & Live Benchmarks\n",
    "\n",
    "Benchmark data evolves as models are updated and new models are released. Here are some sources that can be used as starting point:\n",
    "\n",
    "#### üèÜ Benchmark Sources\n",
    "\n",
    "| Resource | Focus | Best For |\n",
    "|----------|-------|----------|\n",
    "| [artificialanalysis.ai](https://artificialanalysis.ai/) | Independent quality benchmarks across multiple models<br>Cost comparisons across providers<br>Speed metrics (tokens/second, time to first token) | Understanding model capabilities with regular updates as new models are released |\n",
    "| [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) | Academic benchmarks (MMLU, HellaSwag, TruthfulQA, etc.) | Understanding model capabilities on standardized tests |\n",
    "| [LMSYS Org Projects](https://lmsys.org/projects/) | Datasets and evaluation tools for large models | Understanding real-world conversational quality |\n",
    "\n",
    "#### ‚ö†Ô∏è Important Notes\n",
    "\n",
    "| Category | Details |\n",
    "|----------|---------|\n",
    "| Benchmark Limitations | ‚Ä¢ Benchmarks are approximations, not guarantees<br>‚Ä¢ Performance varies based on prompt, region, and load<br>‚Ä¢ Your specific use case may differ from benchmark scenarios<br>‚Ä¢ Always test with your own data before production deployment |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Are You Ready to Test?\n",
    "\n",
    "Now that you understand how to interpret benchmarks and where to find current data, let's move to hands-on testing with real AWS Bedrock models. \n",
    "\n",
    "The next sections will show you:\n",
    "\n",
    "1. **Pricing comparison** - See actual costs for your use case\n",
    "2. **Performance metrics** - Measure latency and throughput in real-time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-pricing-header",
   "metadata": {},
   "source": [
    "## üí∞ Step 3: Hands-On Pricing Analysis\n",
    "\n",
    "This section helps you calculate costs for your specific use case. Understanding the economics is crucial for making informed decisions, especially when deploying at scale.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How the AWS Bedrock pricing translates into monthly costs for your use case\n",
    "- What \"good\" cost looks like for different scenarios\n",
    "- Cost optimization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16dc56",
   "metadata": {},
   "source": [
    "**üí≠ Think about:** Before we reveal the pricing data, which model do you think will be the cheapest for a chatbot use case (100 input + 200 output tokens per request)? Does model size always correlate with cost?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31686c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the pricing data (silent mode)\n",
    "from extract_bedrock_pricing import extract_bedrock_model_pricing\n",
    "pricing_data, model_mapping, bedrock_pricing_json = extract_bedrock_model_pricing(verbose=False)\n",
    "\n",
    "# Step 2: Convert to pandas DataFrame\n",
    "print(\"Converting to DataFrame...\")\n",
    "df = pd.DataFrame.from_dict(bedrock_pricing_json, orient='index')\n",
    "\n",
    "# Step 3: Clean up the DataFrame\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'model_id', \n",
    "                   'input': '$/1M input tokens', \n",
    "                   'output': '$/1M output tokens'}, inplace=True)\n",
    "\n",
    "# Step 5: Add derived columns\n",
    "df['provider'] = df['model_id'].str.split('.').str[0]\n",
    "\n",
    "# Step 4: Reorder columns\n",
    "df = df[['provider', 'name', '$/1M input tokens', '$/1M output tokens', 'model_id', 'region']]\n",
    "\n",
    "print(f\"‚úÖ DataFrame ready! Shape (rows, columns): {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the table with model provider names, model names, prices per token, model ids and primary region\n",
    "# The table can be sorted by any column to help you find the best model for your needs\n",
    "\n",
    "# Filter to workshop models only\n",
    "workshop_models = [\n",
    "    'DeepSeek DeepSeek V3.1',\n",
    "    'Meta Llama 4 Maverick 17B',\n",
    "    'Meta Llama 4 Scout 17B',\n",
    "    'OpenAI gpt-oss-120b',\n",
    "    'OpenAI gpt-oss-20b',\n",
    "    'Qwen Qwen3 235B A22B 2507',\n",
    "    'Qwen Qwen3 32B',\n",
    "    'Qwen Qwen3 Coder 480B A35B'\n",
    "]\n",
    "\n",
    "df_workshop = df[df['name'].isin(workshop_models)].copy()\n",
    "\n",
    "print(\"\\nüí° Sorting Tips:\")\n",
    "print(\"   ‚Ä¢ Sort by output cost (default): df.sort_values('$/1M output tokens', ascending=False)\")\n",
    "print(\"   ‚Ä¢ Sort by input cost: df.sort_values('$/1M input tokens')\")\n",
    "print(\"   ‚Ä¢ Sort by provider: df.sort_values('provider')\")\n",
    "print(\"   ‚Ä¢ Sort by model name: df.sort_values('name')\")\n",
    "print(\"   ‚Ä¢ Multiple columns: df.sort_values(['provider', '$/1M output tokens'])\\n\")\n",
    "\n",
    "# Default view: sorted by output token cost (most expensive first)\n",
    "df_workshop.sort_values('$/1M output tokens', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-case-cost-analysis-header",
   "metadata": {},
   "source": [
    "### üìä Use Case Cost Analysis\n",
    "\n",
    "Raw pricing per million tokens is useful, but what does it mean for your actual application? Let's calculate the monthly costs for a realistic scenario:\n",
    "\n",
    "**Scenario**: 1 million requests per month\n",
    "- **Chatbot**: 100 input tokens + 200 output tokens per request\n",
    "- **Agent**: 500 input tokens + 300 output tokens per request (multi-step reasoning)\n",
    "- **Document Analysis**: 50,000 input tokens + 500 output tokens per request\n",
    "\n",
    "Let's see how costs compare across models for each use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "use-case-cost-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define use case scenarios (tokens per request)\n",
    "use_cases = {\n",
    "    'Chatbot': {'input': 100, 'output': 200},\n",
    "    'Agent': {'input': 500, 'output': 300},\n",
    "    'Document Analysis': {'input': 50000, 'output': 500}\n",
    "}\n",
    "\n",
    "# Number of requests per month\n",
    "requests_per_month = 1_000_000\n",
    "\n",
    "# Calculate costs for each use case\n",
    "for use_case_name, tokens in use_cases.items():\n",
    "    # Cost per request = (input_tokens * input_price + output_tokens * output_price) / 1M\n",
    "    df_workshop[f'{use_case_name} ($/month)'] = (\n",
    "        (tokens['input'] * df_workshop['$/1M input tokens'] + \n",
    "         tokens['output'] * df_workshop['$/1M output tokens']) / 1_000_000\n",
    "    ) * requests_per_month\n",
    "\n",
    "# Create summary table\n",
    "cost_summary = df_workshop[['name'] + [f'{uc} ($/month)' for uc in use_cases.keys()]].copy()\n",
    "cost_summary = cost_summary.sort_values('Chatbot ($/month)')\n",
    "\n",
    "print(\"\\nüí∞ Monthly Cost Comparison (1M requests/month)\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüí° Sorting Tips:\")\n",
    "print(\"   ‚Ä¢ Sort by chatbot cost: cost_summary.sort_values('Chatbot ($/month)')\")\n",
    "print(\"   ‚Ä¢ Sort by agent cost: cost_summary.sort_values('Agent ($/month)')\")\n",
    "print(\"   ‚Ä¢ Sort by document analysis cost: cost_summary.sort_values('Document Analysis ($/month)')\")\n",
    "print(\"   ‚Ä¢ Sort by model name: cost_summary.sort_values('name')\\n\")\n",
    "cost_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-interpretation-guide",
   "metadata": {},
   "source": [
    "### üéØ Cost Optimization Guide\n",
    "\n",
    "Understanding how to optimize for cost depends mostly on your use case and business model. Use this table and tips to evaluate costs optimization strategies for your specific scenario:\n",
    "\n",
    "| Use Case | Key Optimization Tips |\n",
    "|----------|----------------------|\n",
    "| üí¨ Chatbot (100 in + 200 out tokens) | Cache common queries<br>Optimize prompt length |\n",
    "| ü§ñ Agent (500 in + 300 out tokens) | Batch tool calls<br>Cache intermediate results<br>Early stopping |\n",
    "| üìÑ Document Analysis (50K in + 500 out tokens) | Use RAG approach<br>Chunk documents<br>Preprocess to extract relevant sections |\n",
    "| üíª Code Generation (100 in + 200 out tokens) | Quality matters most<br>Specialized models worth premium |\n",
    "| üñºÔ∏è Multimodal (image + text) | Resize images<br>Compress without quality loss<br>Batch requests |\n",
    "\n",
    "**üí° General Cost Optimization Strategies:**\n",
    "1. Right-size your model (don't use 120B when 20B will do)\n",
    "2. Optimize prompts (shorter, clearer = fewer tokens)\n",
    "3. Use streaming to improve UX without cost increase\n",
    "4. Monitor usage and set max token limits\n",
    "5. A/B test to validate expensive models provide value\n",
    "\n",
    "**üéØ Next Steps**: Now that you understand the costs, let's measure actual performance metrics to validate your model choice!\n",
    "\n",
    "**üí≠ Think about:** Before we run the performance tests, make your predictions: Which model will have the fastest Time to First Token? Which will be cheapest per request? Which will have the highest throughput?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742c172",
   "metadata": {},
   "source": [
    "## üí∞ Step 4: Hands-On Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_compare_jupyter_clean import compare_models_simple\n",
    "\n",
    "# pick your favorite model ids from the table above\n",
    "models = [\n",
    "    \"deepseek.v3-v1:0\",\n",
    "    \"openai.gpt-oss-120b-1:0\",\n",
    "    \"openai.gpt-oss-20b-1:0\",\n",
    "    \"meta.llama4-maverick-17b-instruct-v1:0\t\",\n",
    "    \"meta.llama4-scout-17b-instruct-v1:0\",\n",
    "    \"qwen.qwen3-235b-a22b-2507-v1:0\",\n",
    "    \"qwen.qwen3-32b-v1:0\",\n",
    "    # \"qwen.qwen3-coder-480b-a35b-v1:0\"\n",
    "    # \"qwen.qwen3-coder-30b-a3b-v1:0\"\n",
    "]\n",
    "df_metrics = compare_models_simple(models, \"Explain machine learning in 100 words\", bedrock_pricing_json, timeout_single_llm_sec=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f10685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the table with model performance metrics\n",
    "# The table can be sorted by any column to help you compare models\n",
    "\n",
    "print(\"\\nüí° Sorting Tips - Try these commands to sort by different metrics:\")\n",
    "print(\"   ‚Ä¢ Sort by cost (default): df_metrics.drop('Response', axis=1).sort_values('Cost_Cents')\")\n",
    "print(\"   ‚Ä¢ Sort by latency: df_metrics.drop('Response', axis=1).sort_values('Latency_s')\")\n",
    "print(\"   ‚Ä¢ Sort by TTFT (fastest first): df_metrics.drop('Response', axis=1).sort_values('TTFT_s')\")\n",
    "print(\"   ‚Ä¢ Sort by throughput (highest first): df_metrics.drop('Response', axis=1).sort_values('Throughput_tokens_per_sec', ascending=False)\")\n",
    "print(\"   ‚Ä¢ Sort by output tokens: df_metrics.drop('Response', axis=1).sort_values('Output_Tokens', ascending=False)\")\n",
    "print(\"   ‚Ä¢ Sort by tokens/word efficiency: df_metrics.drop('Response', axis=1).sort_values('Tokens_Per_Word')\\n\")\n",
    "\n",
    "# Available columns for sorting:\n",
    "# Cost_Cents - Latency_s - TTFT_s - Input_Tokens - Output_Tokens - Total_Tokens\n",
    "# Word_Count - Tokens_Per_Word - Throughput_tokens_per_sec - Throughput_words_per_sec\n",
    "# Pricing_Type - Input_Rate_Per_1M - Output_Rate_Per_1M\n",
    "\n",
    "#df_metrics.drop('Response', axis=1).sort_values('Cost_Cents', ascending=True)\n",
    "df_metrics.drop(['Response', 'Pricing_Type', 'Input_Rate_Per_1M', 'Output_Rate_Per_1M'], axis=1).sort_values('TTFT_s', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-interpretation-by-use-case",
   "metadata": {},
   "source": [
    "### üéØ Performance Interpretation by Use Case\n",
    "\n",
    "Understanding what \"good\" performance looks like depends on your use case. Use this comprehensive table to interpret the metrics above and identify which models meet your requirements:\n",
    "\n",
    "| Use Case | Priorities | Reference TTFT | Reference Throughput | Reference Latency | Why These Metrics Matter |\n",
    "|----------|---------------|-------------|-------------------|----------------|--------------------------|\n",
    "| üí¨ Chatbot | TTFT > Throughput > Latency | 500ms-1s | 50-100 tokens/s | 2-6s | - Users perceive <500ms TTFT as instant<br>- High throughput improves streaming responsiveness<br>- Cost matters at scale. |\n",
    "| ü§ñ Agent | Quality > Tool Calling > Throughput | 1-3s | 25-50 tokens/s | 5-15s | - Accuracy more important than speed for multi-step workflows<br>- Acceptable to be slower since agents run in background<br>- Higher cost justified for better reasoning |\n",
    "| üìÑ Document Analysis | Context > Quality > Cost | 2-10s | 25-50 tokens/s | 10-60s | - Context window must fit document size<br>- Latency less critical for batch processing<br>- Accuracy in extraction is paramount. |\n",
    "| üíª Code Generation | Code Quality > Context > Speed | 1-4s | 25-50 tokens/s | 5-20s | - Correctness and syntax accuracy matter most<br>- Larger context helps understand full codebases<br>- Faster generation improves developer experience |\n",
    "| üñºÔ∏è Multimodal | Multimodal Support > Quality > Speed | 1-5s | 50-100 tokens/s | 3-10s | - Images add 100-1000+ tokens depending on resolution<br>- Visual understanding accuracy is critical |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä How to Use This Table\n",
    "\n",
    "1. **Find your use case** in the leftmost column\n",
    "2. **Check priority order** to understand what matters most\n",
    "3. **Compare your results** from the performance metrics above against the target thresholds\n",
    "4. **Review recommended models** for your use case\n",
    "5. **Read the rationale** to understand why these metrics matter\n",
    "\n",
    "**üí° Pro Tip**: The \"best\" model depends on your specific requirements. A model that's \"poor\" for chatbots might be \"excellent\" for document analysis. Always prioritize the metrics that matter most for YOUR use case.\n",
    "\n",
    "**Key Insights**:\n",
    "- **Chatbots**: Speed is king - users expect instant responses\n",
    "- **Agents**: Quality over speed - accuracy matters more than raw performance\n",
    "- **Document Analysis**: Context window is critical - must fit your document size\n",
    "- **Code Generation**: Specialized models worth the premium for accuracy\n",
    "- **Multimodal**: Limited options - Llama 4 Maverick is currently the only choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "audience-engagement-1",
   "metadata": {},
   "source": [
    "**üí≠ Think about:** Based on what you just saw, which factor is most important for your use case? Fastest response time, lowest cost, best response quality, or largest context window?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-selection-matrix",
   "metadata": {},
   "source": [
    "## üå≥ Quick Decision Tree\n",
    "\n",
    "- **üìä Need multimodal (image + text)?** ‚Üí Llama 4 Maverick\n",
    "- **üìÑ Processing very long documents?** ‚Üí Llama 4 Scout (3.5M context)\n",
    "- **ü§ñ Building complex agents/workflows?** ‚Üí GPT OSS 120B or DeepSeek V3.1\n",
    "- **‚ö° Need fastest responses?** ‚Üí GPT OSS 20B\n",
    "- **üí∞ Budget is primary concern?** ‚Üí GPT OSS 20B\n",
    "- **üéì Need step-by-step reasoning?** ‚Üí DeepSeek V3.1 or GPT OSS 120B or Qwen3 235B (thinking mode)\n",
    "- **üè¢ Enterprise general-purpose?** ‚Üí Qwen3 32B Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lab1a-closing",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed Lab 1a and learned how to:\n",
    "- ‚úÖ Identify your use case and requirements\n",
    "- ‚úÖ Match models to your specific needs\n",
    "- ‚úÖ Interpret benchmark data in context\n",
    "- ‚úÖ Analyze pricing and performance metrics\n",
    "\n",
    "**Next**: Continue to **[Lab 1b](Lab1b_-_API_Integration_Options.ipynb)** to explore different API options (Invoke, Converse, ChatCompletions) for integrating these models into your applications.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
