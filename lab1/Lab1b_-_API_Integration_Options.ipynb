{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lab1b-header",
   "metadata": {},
   "source": [
    "# re:Invent 2025 - AIM311: Optimize Open Weight Models for Low-Latency, Cost-Effective AI Apps\n",
    "\n",
    "## Lab 1b: API Integration Options\n",
    "**Duration**: 15 minutes  \n",
    "**Focus**: Understanding and using different APIs to call Bedrock models\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this lab, you'll explore three different ways to call models on Amazon Bedrock:\n",
    "\n",
    "1. **Invoke API** - Low-level control for production systems\n",
    "2. **Converse API** - Bedrock-native multi-turn conversations\n",
    "3. **ChatCompletions API** - OpenAI-compatible for easy migration\n",
    "\n",
    "Each API has different strengths and use cases. By the end of this lab, you'll understand when to use each approach.\n",
    "\n",
    "**üí≠ Think about:** If you're building a new application from scratch, which API would you choose? Consider your team's experience level, whether you're migrating from OpenAI, and if you need Bedrock-specific features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-comparison-demo",
   "metadata": {},
   "source": [
    "## API Decision Table\n",
    "\n",
    "Amazon Bedrock offers three different APIs for calling models. Here's a comparison to help you choose the right one for your use case:\n",
    "\n",
    "| Factor | Invoke API | Converse API | ChatCompletions |\n",
    "|--------|-----------|--------------|------------------|\n",
    "| **Learning Curve** | High - Manual JSON handling | Medium - Bedrock-native patterns | Low - Familiar OpenAI patterns |\n",
    "| **Model Support** | All Bedrock models | All Bedrock models | Limited to compatible models |\n",
    "| **Control Level** | Maximum - Direct payload access | High - Structured interface | Medium - Abstracted interface |\n",
    "| **Built-in Features** | None - Manual implementation | Native tool calling, guardrails, streaming | Standard OpenAI features |\n",
    "| **Conversation Management** | Manual | Automatic message history | Automatic message history |\n",
    "| **Type Safety** | No - JSON strings | Yes - Structured objects | Yes - SDK types |\n",
    "| **Debugging** | Easy - Full visibility | Good - Structured errors | Good - SDK error handling |\n",
    "| **Migration Effort** | N/A | Medium - Learn Bedrock patterns | Low - Drop-in OpenAI replacement |\n",
    "| **Ecosystem Compatibility** | Custom integration needed | Large ecosystem based on Bedrock-specific implementations | Very large ecosystem based on OpenAI-specific implementations |\n",
    "| **Performance** | Fastest - Minimal overhead | Fast - Optimized for Bedrock | Fast - SDK optimized |\n",
    "\n",
    "\n",
    "### Quick Selection Guide\n",
    "\n",
    "| API type | Use Cases |\n",
    "|-----|-----------|\n",
    "| Invoke API | Maximum control over request/response payloads<br>Building custom integrations<br>Fastest performance with minimal overhead<br>Access to model-specific parameters |\n",
    "| Converse API | Building Bedrock-native applications<br>Multi-turn conversations or tool calling<br>Using Bedrock guardrails<br>Structured, type-safe interfaces |\n",
    "| ChatCompletions API | Migrating from OpenAI<br>Using existing OpenAI-compatible libraries<br>Rapid prototyping with familiar patterns<br>Using LangChain, LlamaIndex, or similar frameworks \n",
    "---\n",
    "\n",
    "### 1. Invoke API - Basic model invocation with low-Level Control\n",
    "\n",
    "**‚úÖ Use for:** Simple request-response patterns, maximum control\n",
    "\n",
    "```python\n",
    "response = bedrock_runtime.invoke_model(\n",
    "   body=json.dumps({\n",
    "       \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "       \"max_tokens\": 200,\n",
    "       \"temperature\": 0.7\n",
    "   }),\n",
    "   modelId=model_id,\n",
    "   accept=\"application/json\",\n",
    "   contentType=\"application/json\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. Converse API - Bedrock native Multi-turn + Tool calling\n",
    "\n",
    "**‚úÖ Use for:** Multi-turn conversations, tool calling, streaming, Bedrock-native apps\n",
    "\n",
    "```python\n",
    "response = bedrock_runtime.converse(\n",
    "   modelId=model_id,\n",
    "   messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "   toolConfig={\"tools\": tools},\n",
    "   inferenceConfig={\"maxTokens\": 200, \"temperature\": 0.7}\n",
    ")\n",
    "```\n",
    "### 3. ChatCompletions API - OpenAI SDK compatible\n",
    "\n",
    "**‚úÖ Use for:** Drop-in OpenAI replacement, existing codebases, rapid prototyping\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "   base_url=\"https://bedrock-runtime.us-west-2.amazonaws.com/model/{model_id}/v1\",\n",
    "   api_key=\"your-aws-credentials\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "   model=model_id,\n",
    "   messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "   stream=True\n",
    ")\n",
    "```\n",
    "\n",
    "**Note:** The API examples below use models identified in the selection framework from earlier labs. Each example includes brief use case context to help you understand when to use each API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-examples-header",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ Part 1: Model-Specific Examples with Bedrock Converse API's\n",
    "\n",
    "Now that you understand how to select and evaluate models, let's explore hands-on examples for each model family. These examples demonstrate the unique capabilities and use cases for each model.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to use each model family's unique features\n",
    "- Practical code examples for different use cases\n",
    "- Best practices for each model type\n",
    "- How to use each API type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e783bfd",
   "metadata": {},
   "source": [
    "#### Setup client\n",
    "\n",
    "First, we setup the Amazon Bedrock client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a690f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "AWS_REGION = \"us-west-2\"\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "print(f\"‚úÖ Credentials: {credentials.access_key[:10]}...\" if credentials else \"‚ùå No credentials\")\n",
    "\n",
    "# Remove OpenAI SDK environment variables if they exist\n",
    "for key in ['OPENAI_API_KEY', 'OPENAI_BASE_URL', 'OPENAI_ORG_ID']:\n",
    "    if key in os.environ:\n",
    "        del os.environ[key]\n",
    "        print(f\"‚úÖ Removed {key}\")\n",
    "\n",
    "print(\"‚úÖ Environment cleaned - ready to use Bedrock with AWS credentials\")\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=AWS_REGION)\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llama-family-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü¶ô Llama 4 Model Family & Amazon Bedrock's Converse API\n",
    "\n",
    "Meta's Llama 4 family offers multimodal capabilities and ultra-long context windows for advanced use cases.\n",
    "\n",
    "### Available Models:\n",
    "- **Llama 4 Maverick**: Multimodal (text + vision) with 1M context\n",
    "- **Llama 4 Scout**: Ultra-long context (3.5M tokens) for massive documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llama-multimodal-use-case",
   "metadata": {},
   "source": [
    "### üñºÔ∏è Use Case: Multimodal Analysis & Document Processing\n",
    "\n",
    "**Best for:** Image analysis, document OCR, visual Q&A, chart interpretation, screenshot understanding\n",
    "\n",
    "### Llama 4 Models - Technical Overview\n",
    "\n",
    "| Feature | Llama 4 Maverick | Llama 4 Scout |\n",
    "|---------|------------------|---------------|\n",
    "| **Parameters** | 400B total (128 experts, 17B active) | 109B total (16 experts, 17B active) |\n",
    "| **Context Window** | 1M tokens | 3.5M tokens |\n",
    "| **Model ID** | `us.meta.llama4-maverick-17b-instruct-v1:0` | `us.meta.llama4-scout-17b-instruct-v1:0` |\n",
    "| **Type** | Multimodal (text + vision) | Text-only with ultra-long context |\n",
    "| **Best For** | Image analysis, visual Q&A | Ultra-long document processing |\n",
    "| **Tool Calling** | ‚úÖ | ‚úÖ |\n",
    "| **Streaming** | ‚úÖ | ‚úÖ |\n",
    "| **Converse API** | ‚úÖ | ‚úÖ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551f506",
   "metadata": {},
   "source": [
    "### Amazon Bedrock's Converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from PIL import Image\n",
    "\n",
    "MODEL_ID = 'us.meta.llama4-maverick-17b-instruct-v1:0'\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "bedrock_client = boto_session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1577dc",
   "metadata": {},
   "source": [
    "### Function to create the messages object with text prompt and image\n",
    "\n",
    "Since we are going to be using the messages object everytime we are going to query an image, we can write a function as follows to create the payload so that it becomes reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fce587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multi_images_messages(question, image_paths):\n",
    "\n",
    "    images_list = []\n",
    "    images_media_type = []\n",
    "    try:\n",
    "        for img in images_path:\n",
    "            with open(img, \"rb\") as image_file:\n",
    "                image_bytes = image_file.read()\n",
    "                images_list.append(image_bytes)\n",
    "            img_1 = Image.open(img)\n",
    "            imgformat = img_1.format\n",
    "            imgformat = imgformat.lower()\n",
    "            images_media_type.append(imgformat)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image file not found at {image_path}\")\n",
    "        image_data = None\n",
    "        image_media_type = None\n",
    "    \n",
    "    messages = [            \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                {                        \n",
    "                    \"text\": question\n",
    "                },\n",
    "                {\n",
    "                    \"image\": {\n",
    "                        \"format\": images_media_type[0],\n",
    "                        \"source\": {\n",
    "                            \"bytes\": images_list[0]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"image\": {\n",
    "                        \"format\": images_media_type[1],\n",
    "                        \"source\": {\n",
    "                            \"bytes\": images_list[1]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Describe the content in these images\"\n",
    "images_path = [\"img/slide-1.png\", \"img/slide-3.png\"]\n",
    "messages = make_multi_images_messages(question, images_path)\n",
    "for img in images_path:\n",
    "    image = Image.open(img)\n",
    "    image.show()\n",
    "try:\n",
    "    # Invoke the SageMaker endpoint\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=MODEL_ID, # MODEL_ID defined at the beginning\n",
    "        messages=messages,\n",
    "        inferenceConfig={\n",
    "        \"maxTokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": .1\n",
    "        },        \n",
    "    )\n",
    "    \n",
    "    # Read the response \n",
    "    print(response['output']['message']['content'][0]['text'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while invoking the endpoint: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6b38c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Part 2: ü§ñ GPT OSS Model Family & Bedrock OpenAI-Compatible API Integration\n",
    "\n",
    "The GPT OSS family provides OpenAI-compatible models optimized for agentic workflows and complex reasoning tasks.\n",
    "\n",
    "### Available Models:\n",
    "- **GPT-OSS-120B**: High-performance reasoning for complex tasks\n",
    "- **GPT-OSS-20B**: Cost-effective model for simpler workloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431bff26",
   "metadata": {},
   "source": [
    "### ü§ñ Use Case: Agentic Workflows & Complex Reasoning\n",
    "\n",
    "**Example Applications:** Multi-agent systems, research tasks, tool-heavy applications, autonomous decision making\n",
    "\n",
    "**Why GPT OSS and OpenAI-Compatible API?**\n",
    "- 100% OpenAI SDK compatible for seamless migration\n",
    "- Enhanced function calling for tool integration\n",
    "- Advanced reasoning capabilities\n",
    "- Native support for LangChain, LangGraph, CrewAI\n",
    "\n",
    "### GPT OSS Models - Technical Overview\n",
    "\n",
    "| Feature | GPT-OSS-120B | GPT-OSS-20B |\n",
    "|---------|--------------|-------------|\n",
    "| **Parameters** | 120B | 20B |\n",
    "| **Model ID** | `openai.gpt-oss-120b-1:0` | `openai.gpt-oss-20b-1:0` |\n",
    "| **Context Window** | 128K tokens | 128K tokens |\n",
    "| **Best For** | Complex reasoning, agentic workflows | Fast inference, cost-effective deployments |\n",
    "| **Tool Calling** | ‚úÖ Enhanced | ‚úÖ Enhanced |\n",
    "| **Streaming** | ‚úÖ | ‚úÖ |\n",
    "| **Converse API** | ‚úÖ | ‚úÖ |\n",
    "| **OpenAI Compatible** | ‚úÖ 100% | ‚úÖ 100% |\n",
    "| **Languages** | Multilingual | Multilingual |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393ad20",
   "metadata": {},
   "source": [
    "### Amazon Bedrock's Chat Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -U boto3 openai langchain langgraph langchain-openai langchain-core langchain-aws aws-bedrock-token-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d63094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "MODEL_ID = \"openai.gpt-oss-120b-1:0\"  # or \"openai.gpt-oss-20b-1:0\" for faster inference\n",
    "REGION = \"us-west-2\"\n",
    "os.environ['AWS_REGION'] = REGION\n",
    "\n",
    "print(f\"‚úÖ Using model: {MODEL_ID}\")\n",
    "print(f\"‚úÖ Region: {REGION}\")\n",
    "print(\"‚úÖ Using AWS credentials from your environment (AWS CLI, env vars, or IAM role)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48d4a1",
   "metadata": {},
   "source": [
    "### Using LangChain's OpenAI wrapper classes natively with Amazon Bedrock\n",
    "\n",
    "LangChain provides a comprehensive framework for building LLM applications. We'll explore how to use LangChain's popular OpenAI wrapper classes with Bedrock's OpenAI compatible endpoint. \n",
    "\n",
    "**Note:** We'll set the environment variables `OPENAI_BASE_URL` and `OPENAI_API_KEY` to redirect traffic to Amazon Bedrock's OpenAI compatible endpoint and handle authentication via AWS credentials. This allows us to use `ChatOpenAI` seamlessly with Bedrock models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI SDK to use Bedrock's OpenAI-compatible endpoint\n",
    "# The OpenAI SDK with Bedrock requires a Bedrock API key (not AWS credentials)\n",
    "# We'll use AWS's official token generator library to create short-term API keys\n",
    "\n",
    "from aws_bedrock_token_generator import provide_token\n",
    "\n",
    "# Generate a short-term Bedrock API key (valid for up to 12 hours)\n",
    "# This automatically uses your AWS credentials and inherits their permissions\n",
    "try:\n",
    "    api_key = provide_token()\n",
    "    print(\"‚úÖ Bedrock API Key generated successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating Bedrock API key: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting tips:\")\n",
    "    print(\"   1. Ensure your AWS credentials are configured (AWS CLI, env vars, or IAM role)\")\n",
    "    print(\"   2. Verify you have permissions to use Amazon Bedrock\")\n",
    "    print(\"   3. Check that you're in a supported region (us-west-2, us-east-1, etc.)\")\n",
    "    print(\"   4. Install the token generator: pip install aws-bedrock-token-generator\")\n",
    "    raise\n",
    "\n",
    "# Set environment variables for OpenAI SDK\n",
    "# IMPORTANT: The base_url should be /openai/v1 (NOT /model/{model_id}/v1)\n",
    "# The model ID is specified in the chat.completions.create() call\n",
    "base_url = f\"https://bedrock-runtime.{REGION}.amazonaws.com/openai/v1\"\n",
    "os.environ['OPENAI_BASE_URL'] = base_url\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "print(f\"‚úÖ OpenAI Base URL: {base_url}\")\n",
    "\n",
    "# Initialize LangChain with ChatOpenAI using Bedrock's OpenAI-compatible endpoint\n",
    "# The environment variables will automatically be picked up by ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL_ID,\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LangChain LLM initialized with model: {MODEL_ID}\")\n",
    "print(\"‚úÖ Using ChatOpenAI (OpenAI-compatible Bedrock endpoint)\")\n",
    "print(f\"‚úÖ API KEY: {api_key[:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c08cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LangChain Chain using LCEL (LangChain Expression Language)\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful coding assistant. Provide clear, concise explanations.\"),\n",
    "    (\"human\", \"Explain the concept of {topic} in programming.\")\n",
    "])\n",
    "\n",
    "# Modern approach: Use pipe operator (LCEL) instead of deprecated LLMChain\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "# Note: invoke() takes a dictionary as input, not keyword arguments\n",
    "response = chain.invoke({\"topic\": \"dependency injection\"})\n",
    "\n",
    "print(\"üíª LangChain Chain Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrap-up-section",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ Key Takeaways & Next Steps\n",
    "\n",
    "### What You've Learned in Lab 1b\n",
    "\n",
    "You've now explored the three main ways to integrate Bedrock models into your applications:\n",
    "\n",
    "1. **Invoke API** - Maximum control for production systems\n",
    "2. **Converse API** - Bedrock-native features like tool calling and guardrails  \n",
    "3. **ChatCompletions API** - OpenAI-compatible for easy migration\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Great Work!\n",
    "\n",
    "You've successfully completed the model selection and API integration foundations. Lab 2 will build on this knowledge to help you optimize and evaluate models for production deployment.\n",
    "\n",
    "**Continue to ‚Üí [Lab 2: Model Evaluation & Optimization](../lab2/Lab2a_-_Automatic_model_evaluation.ipynb)**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
