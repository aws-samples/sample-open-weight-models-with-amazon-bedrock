{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c73fd5-4e9d-4e42-ae60-8dc50c9dc73c",
   "metadata": {},
   "source": [
    "# Evaluate LLMs performance by metrics using Amazon Bedrock Automatic Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6f511-21f5-4c5c-ac9c-d6d0afc374d2",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to evaluate Large Language Models (LLMs) using Amazon Bedrock's Automatic Model Evaluation (AME) capabilities. By the end of this notebook, you will understand how to set up, run, and interpret various metrics-based evaluations to assess model performance across different dimensions.\n",
    "\n",
    "For supported regions and models please refer to https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-support.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14992a89-c56b-413f-930e-8a47519482c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T04:20:53.122597Z",
     "iopub.status.busy": "2025-10-21T04:20:53.122341Z",
     "iopub.status.idle": "2025-10-21T04:20:53.126227Z",
     "shell.execute_reply": "2025-10-21T04:20:53.125221Z",
     "shell.execute_reply.started": "2025-10-21T04:20:53.122578Z"
    }
   },
   "source": [
    "## Automatic model evaluations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658e145-f83e-4199-92c0-1efc2d5a12f7",
   "metadata": {},
   "source": [
    "\n",
    "Automatic model evaluation jobs allow you to quickly assess a model's ability to perform specific tasks with minimal setup. By using AME, you can systematically compare model performance across different dimensions, make data-driven decisions about model selection, and identify opportunities for prompt engineering improvements.\n",
    "\n",
    "You can either provide your own custom prompt dataset tailored to your specific use case, or leverage Amazon's built-in datasets for standardized evaluations.\n",
    "\n",
    "\n",
    "\n",
    "### Key Benefits of Bedrock AME\n",
    "\n",
    "**Streamlined Evaluation Process:** Evaluate model performance without building custom evaluation infrastructure\n",
    "\n",
    "**Flexible Dataset Options:** Use built-in datasets or customize your own evaluation prompts\n",
    "\n",
    "**Comprehensive Metrics:** Access industry-standard metrics for different LLM capabilities\n",
    "\n",
    "**Multi-Model Comparison:** Easily benchmark performance across different models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8fee1-9f8f-4b52-97bc-d44aae8ad9db",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "Before proceeding with this lab on model evaluation, you need to complete some pre-requisites.\n",
    "Later in this notebook, you will have the opportunity to go through these steps in detail. Please take some time to undrestand to review them as it will help you implement similar evaluations in your own AWS environment later.\n",
    "\n",
    "#### Required Resources and Permissions\n",
    "1. Amazon S3 Storage Configuration\n",
    "    * Regional Compatibility: An Amazon S3 bucket must exist in the same AWS Region as your Amazon Bedrock models\n",
    "        * Example: When using Bedrock in us-west-2, your S3 bucket must also be in us-west-2\n",
    "    * CORS Configuration: The S3 bucket requires Cross Origin Resource Sharing (CORS) configuration enabled\n",
    "        * This allows proper communication between Amazon Bedrock services and your storage.\n",
    "\n",
    "\n",
    "2. IAM Role Requirements\n",
    "The IAM role executing this notebook must have sufficient permissions to perform the following:\n",
    "    * S3 Operations:\n",
    "        * Read from and write to your designated Amazon S3 bucket\n",
    "        * Upload evaluation datasets and retrieve results\n",
    "    * Bedrock Service Access:\n",
    "        * Invoke Amazon Bedrock foundation models\n",
    "        * Create and manage model inference configurations\n",
    "    * Evaluation Job Management:\n",
    "       * Create and initiate evaluation jobs\n",
    "       * Monitor job status and progress\n",
    "       * Access and download evaluation results\n",
    "         \n",
    "For a comprehensive list of prerequisites and detailed setup instructions for your own environment, please refer to the [official Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-automatic.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201c70e-b66f-4f0e-8ab1-1f61f8357a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T04:40:15.602060Z",
     "iopub.status.busy": "2025-10-21T04:40:15.601789Z",
     "iopub.status.idle": "2025-10-21T04:40:15.605125Z",
     "shell.execute_reply": "2025-10-21T04:40:15.604341Z",
     "shell.execute_reply.started": "2025-10-21T04:40:15.602039Z"
    }
   },
   "source": [
    "\n",
    "## Environment Setup\n",
    "The following code installs and upgrades the necessary Python libraries required for this notebook. We'll ensure that all dependencies are at their latest compatible versions to avoid any unexpected issues.\n",
    "\n",
    "### Required Dependencies\n",
    "\n",
    "| Package | Description |\n",
    "|---------|------------|\n",
    "| `awscli` | AWS Command Line Interface tools for AWS services interaction |\n",
    "| `boto3` | AWS SDK for Python - enables programmatic AWS service access |\n",
    "| `seaborn` | Statistical data visualization built on matplotlib |\n",
    "| `matplotlib` | Comprehensive library for creating visualizations |\n",
    "| `sagemaker` | Amazon SageMaker Python SDK for ML workflows |\n",
    "\n",
    "\n",
    "The --quiet flag reduces installation output to keep the notebook clean, while --upgrade ensures we're using the latest versions of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113232e-5f9e-417e-85b6-0916dcab9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9ee10-6a2e-4a16-9834-b87ab275d1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T18:34:00.173187Z",
     "iopub.status.busy": "2025-10-28T18:34:00.172922Z",
     "iopub.status.idle": "2025-10-28T18:34:00.176190Z",
     "shell.execute_reply": "2025-10-28T18:34:00.175477Z",
     "shell.execute_reply.started": "2025-10-28T18:34:00.173167Z"
    }
   },
   "source": [
    "## Complete Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f0295-5fde-4441-b4b2-ae393dc02d0e",
   "metadata": {},
   "source": [
    "### Choose a S3 Bucket for Model Evaluation jobs\n",
    "----\n",
    "\n",
    "Bedrock model evaluation jobs require an Amazon S3 bucket in your current AWS region to store input datasets and model evaluation results.\n",
    "\n",
    "\n",
    "**If you're running this notebook in the JupyterLab environment in Amazon SageMaker AI Studio, you can use your the default bucket from the Amazon SageMaker session to store the datasets and evaluation results. To do so, run the code below as-is.** For users running this notebook outside SageMaker AI Studio (for example on a local machine or EC2 instance), you'll need to either create a new S3 bucket or specify an existing one in your region. Please follow the instructions within the cell before you execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d253e-c2d8-4c81-afbd-ad4d4ad25bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker #comment if Sagemaker is not used\n",
    "import boto3\n",
    "sess = sagemaker.Session() #comment if Sagemaker is not used\n",
    "\n",
    "#If you want to use a custom s3 bucket or running this notebook without Sagemaker, please mention the bucket name as follows\n",
    "#bucket = \"\"\n",
    "bucket=None\n",
    "\n",
    "if bucket is None and sess is not None: \n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    " \n",
    "\n",
    "print(f\"Model Evaluation bucket: {bucket}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0cf05d-a2d2-4a66-b439-112c38b14e75",
   "metadata": {},
   "source": [
    "### Enable Cross Origin Resource Sharing (CORS) on S3 bucket\n",
    "----\n",
    "\n",
    "Automatic model evaluations jobs that are created using the Amazon Bedrock console require that you specify a CORS configuration on the S3 bucket you use to store the datsets and model evaluation results.\n",
    "\n",
    "Please refer to https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security-cors.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c8718-44e1-4563-85dd-e122d79a91fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cors\n",
    "# Define the configuration rules\n",
    "cors_configuration = {\n",
    "    'CORSRules': [\n",
    "    {\n",
    "        \"AllowedHeaders\": [\n",
    "            \"*\"\n",
    "        ],\n",
    "        \"AllowedMethods\": [\n",
    "            \"GET\",\n",
    "            \"PUT\",\n",
    "            \"POST\",\n",
    "            \"DELETE\"\n",
    "        ],\n",
    "        \"AllowedOrigins\": [\n",
    "            \"*\"\n",
    "        ],\n",
    "        \"ExposeHeaders\": [\n",
    "            \"Access-Control-Allow-Origin\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "}\n",
    "\n",
    "# Set the CORS configuration\n",
    "s3 = boto3.client('s3')\n",
    "s3.put_bucket_cors(Bucket=bucket,\n",
    "                   CORSConfiguration=cors_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845fe74-b49a-408a-ab46-35bc16bef555",
   "metadata": {},
   "source": [
    "### IAM service role\n",
    "\n",
    "To run an automatic model evaluation job you must create a service role. The service role allows Amazon Bedrock to perform actions on your behalf in your AWS account. Please refer to https://docs.aws.amazon.com/bedrock/latest/userguide/automatic-service-roles.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e24f18-7c02-4b30-b7c2-7d8ac92498a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#Create IAM role\n",
    "iam = boto3.client('iam')\n",
    "aws_acct = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "assume_role_policy_document = json.dumps({\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllowBedrockToAssumeRole\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": aws_acct\n",
    "                },\n",
    "                \"ArnEquals\": {\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:{}:{}:evaluation-job/*\".format(region, aws_acct)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da5d0d-d629-46ce-883e-8ec00571ee0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "role_name=\"Amazon-Bedrock-model-eval-{}\".format(str(datetime.datetime.now().timestamp()).split('.')[0])\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument = assume_role_policy_document\n",
    ")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66b475-ad07-424f-9ce4-665d435c1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Waiter function to check if IAM role got successfully created\n",
    "waiter = iam.get_waiter('role_exists')\n",
    "print(f\"Waiting for role '{role_name}' to exist...\")\n",
    "\n",
    "# Wait for the role to exist\n",
    "waiter.wait(\n",
    "    RoleName=role_name,\n",
    "    WaiterConfig={\n",
    "        'Delay': 2,        # Optional: Poll every 2 seconds instead of 1\n",
    "        'MaxAttempts': 5  # Optional: Max attempts 30 times instead of 20\n",
    "    }\n",
    ")\n",
    "print(f\"Role '{role_name}' found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30c1a8-3797-424d-a00d-8c3326226154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "\n",
    "role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6dd6f9-b2b6-42c1-97b9-8820b35f42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store role_arn and role_name for reuse in lab2b\n",
    "%store role_arn\n",
    "%store role_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2619f82-8ee0-4869-9851-1a960bf801e3",
   "metadata": {},
   "source": [
    "### Add Permissions to IAM role to access Amazon Bedrock and the Amazon S3 Bucket\n",
    "---\n",
    "Next, you need to allow the IAM service role to access to the S3 bucket you specified and Bedrock capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4bd66-eae4-4b71-8709-d53244001b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aws_s3_policy_doc = json.dumps({\n",
    "\"Version\": \"2012-10-17\",\n",
    "\"Statement\": [\n",
    "    {\n",
    "        \"Sid\": \"AllowAccessToCustomDatasetsAndOutput\",\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"s3:GetObject\",\n",
    "            \"s3:ListBucket\",\n",
    "            \"s3:PutObject\"\n",
    "        ],\n",
    "        \"Resource\": [\n",
    "            \"arn:aws:s3:::{}\".format(bucket),\n",
    "            \"arn:aws:s3:::{}/outputs/\".format(bucket),\n",
    "            \"arn:aws:s3:::{}/custom_datasets/\".format(bucket),\n",
    "            \"arn:aws:s3:::{}/*\".format(bucket),\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "}\n",
    ")\n",
    "\n",
    "aws_br_policy_doc = json.dumps({\n",
    "        \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllowAccessToBedrockResources\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:InvokeModel\",\n",
    "                \"bedrock:InvokeModelWithResponseStream\",\n",
    "                \"bedrock:CreateModelInvocationJob\",\n",
    "                \"bedrock:StopModelInvocationJob\",\n",
    "                \"bedrock:GetProvisionedModelThroughput\",\n",
    "                \"bedrock:GetInferenceProfile\", \n",
    "                \"bedrock:ListInferenceProfiles\",\n",
    "                \"bedrock:GetImportedModel\",\n",
    "                \"bedrock:GetPromptRouter\",\n",
    "                \"bedrock:GetEvaluationJob\",\n",
    "                \"bedrock:ListEvaluationJobs\",\n",
    "                \"bedrock:CreateEvaluationJob\",\n",
    "                \"sagemaker:InvokeEndpoint\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:bedrock:*::foundation-model/*\",\n",
    "                \"arn:aws:bedrock:*:{}:inference-profile/*\".format(aws_acct),\n",
    "                \"arn:aws:bedrock:*:{}:provisioned-model/*\".format(aws_acct),\n",
    "                \"arn:aws:bedrock:*:{}:imported-model/*\".format(aws_acct),\n",
    "                \"arn:aws:bedrock:*:{}:application-inference-profile/*\".format(aws_acct),\n",
    "                \"arn:aws:bedrock:*:{}:default-prompt-router/*\".format(aws_acct),\n",
    "                \"arn:aws:sagemaker:*:{}:endpoint/*\".format(aws_acct),\n",
    "                \"arn:aws:bedrock:*:{}:marketplace/model-endpoint/all-access\".format(aws_acct)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f4b73-6bf3-44dc-966c-19f524264f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_policy_propagation(iam_client, role_name, policy_name):\n",
    "    for attempt in range(30):\n",
    "        try:\n",
    "            iam_client.get_role_policy(RoleName=role_name, PolicyName=policy_name)\n",
    "            return\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "                print(\"NoSuchEntity, trying again\")\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44619227-8ac0-4d81-ae47-08ce061223b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iam_s3_response = iam.put_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyName=\"s3_access\",\n",
    "    PolicyDocument=aws_s3_policy_doc\n",
    ")\n",
    "iam_s3_response\n",
    "wait_for_policy_propagation(iam, role_name, \"s3_access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55969bd-b158-4eb5-9aef-4b516d11a819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iam_bedrock_response = iam.put_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyName=\"br_access\",\n",
    "    PolicyDocument=aws_br_policy_doc\n",
    ")\n",
    "iam_bedrock_response\n",
    "wait_for_policy_propagation(iam, role_name, \"br_access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6a0fa-acad-4942-8ca8-99c397d2fa53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T18:24:53.260186Z",
     "iopub.status.busy": "2025-10-28T18:24:53.259935Z",
     "iopub.status.idle": "2025-10-28T18:24:53.264921Z",
     "shell.execute_reply": "2025-10-28T18:24:53.264029Z",
     "shell.execute_reply.started": "2025-10-28T18:24:53.260165Z"
    }
   },
   "source": [
    "# Run model evaluation job\n",
    "## Model Selection\n",
    "\n",
    "In this step, we'll compare two powerful Large Language Models (LLMs) available through Amazon Bedrock:\n",
    "\n",
    "\n",
    "### 1. Qwen-3 32B (Alibaba)\n",
    "- **Bedrock Model ID:** `qwen.qwen3-32b-v1:0`\n",
    "\n",
    "### 2. GPT OSS 20B (OpenAI)\n",
    "- **Bedrock Model ID:** `openai.gpt-oss-20b-1:0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf478a3-6a2a-4daa-a6dc-c78c7cab5771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T18:27:47.870440Z",
     "iopub.status.busy": "2025-10-28T18:27:47.870180Z",
     "iopub.status.idle": "2025-10-28T18:27:47.875640Z",
     "shell.execute_reply": "2025-10-28T18:27:47.874697Z",
     "shell.execute_reply.started": "2025-10-28T18:27:47.870417Z"
    }
   },
   "source": [
    "You can list the available models and retreive their model ids using the following code\n",
    "\n",
    "```\n",
    "import boto3\n",
    "bedrock_client = boto3.client('bedrock')\n",
    "bedrock_client.list_foundation_models()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576c6a0-52e0-4539-ac13-829f96a0b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client('bedrock', region_name=region)\n",
    "gen_models = [ \"qwen.qwen3-32b-v1:0\", \"openai.gpt-oss-20b-1:0\"]\n",
    "model_1 = gen_models[0]\n",
    "model_2 = gen_models[1]\n",
    "print('You selected models {} and {} for evaluation'.format(model_1, model_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392f953-1203-4fea-8566-f20c1faea19a",
   "metadata": {},
   "source": [
    "#### Lets get details about the selected Amazon Bedrock foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652da94f-6c0e-46c8-bef4-fc698b07ccf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client.get_foundation_model(modelIdentifier=model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610f7c5-c67d-44fb-9774-875592aae9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client.get_foundation_model(modelIdentifier=model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f4783-8e87-4bc3-bb8c-90282c9fa977",
   "metadata": {},
   "source": [
    "#### Get ARNs for the selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59fd19-8bf4-48f5-ac25-4756088c875e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "bedrock_client = boto3.client('bedrock')\n",
    "region = boto3.session.Session().region_name\n",
    "region_prefix = region.split('-')[0]\n",
    "model_arns = []\n",
    "\n",
    "\n",
    "\n",
    "for model in model_1, model_2:\n",
    "    fm_response = bedrock_client.get_foundation_model(\n",
    "        modelIdentifier=model\n",
    "    )\n",
    "    if fm_response['modelDetails']['inferenceTypesSupported'][0] == \"ON_DEMAND\":\n",
    "        model_arn = fm_response['modelDetails']['modelArn']\n",
    "    elif fm_response['modelDetails']['inferenceTypesSupported'][0] == \"INFERENCE_PROFILE\":\n",
    "        model = \"{}.{}\".format(region_prefix, model)\n",
    "        model_arn = bedrock_client.get_inference_profile(\n",
    "            inferenceProfileIdentifier=model\n",
    "        )['inferenceProfileArn']\n",
    "    model_arns.append(model_arn)\n",
    "\n",
    "print(model_arns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf97e5-7a1a-42cd-a3f5-82605a03cd80",
   "metadata": {},
   "source": [
    "# <ins> Automatic Model evaluation using Builtin Dataset </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d0456-c77f-4509-ae80-aadf65ec6216",
   "metadata": {},
   "source": [
    "### Define taskType, Dataset and metrics for evaluation\n",
    "\n",
    "**Task Type:**\n",
    "Model evaluation supports the following task types that assess different aspects of the model's performance:\n",
    "\n",
    "* General text generation ‚Äì the model performs natural language processing and text generation tasks.\n",
    "* Text summarization ‚Äì the model performs summarizes text based on the prompts you provide.\n",
    "* Question and answer ‚Äì the model provides answers based on your prompts.\n",
    "* Text classification ‚Äì the model categorizes text into predefined classes based on the input dataset.\n",
    "\n",
    "**Metrics:**\n",
    "You can choose from the following the metrics that you want the model evaluation job to create.\n",
    "\n",
    "* Toxicity ‚Äì The presence of harmful, abusive, or undesirable content generated by the model.\n",
    "* Accuracy ‚Äì The model's ability to generate outputs that are factually correct, coherent, and aligned with the intended task or query.\n",
    "* Robustness ‚Äì The model's ability to maintain consistent and reliable performance in the face of various types of challenges or perturbations.\n",
    "\n",
    "**Datasets:**\n",
    "Amazon Bedrock provides multiple built-in prompt datasets that you can use in an automatic model evaluation job. Each built-in dataset is based off an open-source dataset. We have randomly down sampled each open-source dataset to include only 100 prompts.\n",
    "\n",
    "For complete list of supported datasets, Task Types and metrics, please refer to https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model_arn, dataset, task_type, output_path, job_name, metric_names, custom_ds=False, custom_ds_s3=None):\n",
    "    if custom_ds:\n",
    "        ds = {\n",
    "                'name': dataset,\n",
    "                'datasetLocation': {\n",
    "                    's3Uri': custom_ds_s3\n",
    "                        }\n",
    "            }\n",
    "    else:\n",
    "        ds = {\n",
    "                'name': dataset\n",
    "            }\n",
    "    job_request = bedrock_client.create_evaluation_job(\n",
    "        jobName=job_name,\n",
    "        jobDescription=\"Bedrock Model evaluation job\",\n",
    "        roleArn=role_arn,\n",
    "        outputDataConfig={\n",
    "            \"s3Uri\": output_path\n",
    "        },\n",
    "        inferenceConfig={\n",
    "            \"models\": [\n",
    "                {\n",
    "                    \"bedrockModel\": {\n",
    "                        \"modelIdentifier\":model_arn,\n",
    "                        \"inferenceParams\":\"{\\\"inferenceConfig\\\":{\\\"maxTokens\\\": 1024,\\\"temperature\\\":0.3,\\\"topP\\\":0.5}}\"\n",
    "                    }\n",
    "\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        },\n",
    "        evaluationConfig={\n",
    "        'automated': {\n",
    "            'datasetMetricConfigs': [\n",
    "                {\n",
    "                    \"taskType\": task_type,\n",
    "                        \"dataset\": ds,\n",
    "                        \"metricNames\": metric_names\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return job_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef811d-c94d-4bbd-be1f-ba2672210906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "### Use any one of the following examples combinations of task_type, dataset and metrics or from supported built-in task_types, metrics and datasets from \n",
    "### https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets.html#model-evaluation-prompt-datasets-builtin\n",
    "\n",
    "#### Example-1 #####\n",
    "task_type = \"QuestionAndAnswer\"\n",
    "dataset = \"Builtin.NaturalQuestions\"\n",
    "metric_names = [\"Builtin.Accuracy\", \"Builtin.Robustness\", \"Builtin.Toxicity\"]\n",
    "\n",
    "#### Example-2 #####\n",
    "#task_type = \"Classification\"\n",
    "#dataset = \"Builtin.WomensEcommerceClothingReviews\"\n",
    "#metric_names = [\"Builtin.Accuracy\", \"Builtin.Robustness\"] \n",
    "\n",
    "output_path = \"s3://{}/outputs/\".format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d06782-9433-46ce-9918-e22f6f1eab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def wait_for_role_propagation(role_arn, max_wait=10):\n",
    "    \"\"\"Wait for IAM role to be assumable by Bedrock service\"\"\"\n",
    "    sts_client = boto3.client('sts')\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < max_wait:\n",
    "        try:\n",
    "            # Test if Bedrock can assume the role by checking if we can get role info\n",
    "            iam_client = boto3.client('iam')\n",
    "            role_name = role_arn.split('/')[-1]\n",
    "            # Check if role exists and policies are attached\n",
    "            role_info = iam_client.get_role(RoleName=role_name)\n",
    "            policies = iam_client.list_role_policies(RoleName=role_name)\n",
    "            if len(policies['PolicyNames']) >= 2:  # Should have s3_access and br_access\n",
    "                print(f\"Role ready after {int(time.time() - start_time)} seconds\")\n",
    "                return True\n",
    "        except ClientError as e:\n",
    "            pass\n",
    "        print(f\"Waiting for role propagation... ({int(time.time() - start_time)}s)\")\n",
    "        time.sleep(5)\n",
    "    #raise TimeoutError\n",
    "    print(f\"Role should be propagated by now, proceeding\")\n",
    "\n",
    "# Wait for role to be ready\n",
    "wait_for_role_propagation(role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80acac96-ee68-4ff6-86d2-1f215a8c21af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_jobs = []\n",
    "for model_arn in model_arns:\n",
    "    job_name = \"model-eval-{}-{}\".format(model_arn.split('/')[-1].split(':')[0], str(datetime.datetime.now().timestamp()).split('.')[0])\n",
    "    job_name = job_name.replace(\".\", \"-\")\n",
    "    job_name\n",
    "    print(job_name)\n",
    "    job = model_eval(model_arn, dataset, task_type, output_path, job_name, metric_names)\n",
    "    eval_jobs.append(job)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3461d-da2a-4856-88b0-99f4172213dd",
   "metadata": {},
   "source": [
    "### Monitoring Bedrock Model Evaluation Jobs\n",
    "\n",
    "This function continuously checks the status of two submitted AWS Bedrock evaluation jobs until they either complete or fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144ddd6-c615-45e9-9b72-5526a4051cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to check the job status in a loop until \"COMPLETED\" or \"FAILED\" post submission.\n",
    "def check_job_status(eval_jobs, loop=True):\n",
    "    # Loop through and wait for the evaluation jobs to complete . \n",
    "    from IPython.display import clear_output\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    \n",
    "    max_time = time.time() + 2*60*60 # 2 hours - Update the max time if needed\n",
    "    \n",
    "    while True:\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        get_eval_job1 = bedrock_client.get_evaluation_job(\n",
    "            jobIdentifier=eval_jobs[0]['jobArn']\n",
    "        )\n",
    "\n",
    "        job1_status = get_eval_job1[\"status\"]\n",
    "        get_eval_job2 = bedrock_client.get_evaluation_job(\n",
    "            jobIdentifier=eval_jobs[1]['jobArn']\n",
    "        )\n",
    "\n",
    "        job2_status = get_eval_job2[\"status\"]\n",
    "        \n",
    "        if loop:\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "        print(f\"{current_time} : Model evaluation job1 is {job1_status} and job2 is {job2_status}.\")\n",
    "\n",
    "        if not loop or (job1_status == \"Completed\" or job1_status == \"Failed\") and (job2_status == \"Completed\" or job2_status == \"Failed\") or time.time() >= max_time:\n",
    "            break\n",
    "\n",
    "        time.sleep(60)\n",
    "    \n",
    "    return get_eval_job1, get_eval_job2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8b034-38eb-46c3-8b86-1d437d5c9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "status1, status2 = check_job_status(eval_jobs, loop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "console-link-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"You can also review the status of the jobs in the [Amazon Bedrock Console](https://{region}.console.aws.amazon.com/bedrock/home?region={region}#/eval/evaluation)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wait-note-cell",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; border-left: 4px solid #28a745; padding: 15px; border-radius: 5px;\">\n",
    "\n",
    "<strong>The evaluation jobs you just submitted may take several minutes to complete.</strong><br><br>\n",
    "\n",
    "\n",
    "Instead of waiting for the submitted evaluation job(s) to complete, let's proceed with monitoring and analyzing results from previously completed jobs. This approach allows us to:\n",
    "\n",
    "‚è±Ô∏è Make productive use of our workshop time.\n",
    "\n",
    "üß† Understand the evaluation framework and metrics.\n",
    "\n",
    "üìà Compare existing model performance results.\n",
    "\n",
    "In the following cells, we'll:\n",
    "\n",
    "üîÑ Check the status of our submitted job(s).\n",
    "\n",
    "üì• Retrieve and analyze results from completed evaluation jobs.\n",
    "\n",
    "‚öñÔ∏è Compare performance across different models.\n",
    "\n",
    "üìä Visualize key metrics and insights.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieve-jobs-header",
   "metadata": {},
   "source": [
    "Next, you retrieve the most recent jobs run for the selected models, task type and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieve-jobs-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "def get_completed_automatic_jobs(custom=False):\n",
    "    all_jobs = []\n",
    "    next_token = None\n",
    "    \n",
    "    # Get all jobs with pagination\n",
    "    while True:\n",
    "        params = {\n",
    "            'sortBy': 'CreationTime',\n",
    "            'sortOrder': 'Descending',\n",
    "            'statusEquals': 'Completed',\n",
    "            'maxResults': 1000\n",
    "        }\n",
    "        \n",
    "        if next_token:\n",
    "            params['nextToken'] = next_token\n",
    "            \n",
    "        response = bedrock_client.list_evaluation_jobs(**params)\n",
    "        all_jobs.extend(response['jobSummaries'])\n",
    "        \n",
    "        next_token = response.get('nextToken')\n",
    "        if not next_token:\n",
    "            break\n",
    "    \n",
    "    print(\"response #\", len(all_jobs))\n",
    "    \n",
    "    jobs = [\n",
    "        job for job in all_jobs\n",
    "        if 'evaluatorModelIdentifiers' not in job\n",
    "        and any(job.get('modelIdentifiers', []) == [model] for model in model_arns)\n",
    "        and job.get('evaluationTaskTypes', []) == [task_type]\n",
    "    ]\n",
    "    \n",
    "    # Group jobs by unique combination of model and dataset type\n",
    "    job_groups = {}\n",
    "    \n",
    "    for job in jobs:\n",
    "        details = bedrock_client.get_evaluation_job(jobIdentifier=job['jobArn'])\n",
    "        dataset_name = details['evaluationConfig']['automated']['datasetMetricConfigs'][0]['dataset']['name']\n",
    "        is_builtin = dataset_name.startswith('Builtin.')\n",
    "        \n",
    "        # Skip if doesn't match the requested type (custom vs builtin)\n",
    "        if (custom and is_builtin) or (not custom and not is_builtin):\n",
    "            continue\n",
    "             \n",
    "        model_id = job['modelIdentifiers'][0]\n",
    "        eval_type = 'builtin' if is_builtin else 'custom'\n",
    "        key = (model_id, eval_type, dataset_name)\n",
    "         \n",
    "        # Keep only the most recent job for each unique combination\n",
    "        if key not in job_groups or job['creationTime'] > job_groups[key]['creationTime']:\n",
    "            job_groups[key] = job\n",
    "    \n",
    "    return list(job_groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c03837-164e-4578-89ad-bd988b5d9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_jobs = get_completed_automatic_jobs(custom=False)\n",
    "\n",
    "if len(completed_jobs) >= 2:\n",
    "    print(f\"Found {len(completed_jobs)} completed jobs. Selecting the latest.\")\n",
    "    get_eval_job1 = bedrock_client.get_evaluation_job(jobIdentifier=completed_jobs[0]['jobArn'])\n",
    "    print(f\"Job1 name: {get_eval_job1['jobName']}\\nDetails: {get_eval_job1}\")\n",
    "    get_eval_job2 = bedrock_client.get_evaluation_job(jobIdentifier=completed_jobs[1]['jobArn'])\n",
    "    print(f\"Job2 name: {get_eval_job2['jobName']}\\nDetails: {get_eval_job2}\")\n",
    "else:\n",
    "    print(f\"Only found {len(completed_jobs)} completed jobs. Need to wait for jobs to complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8f3d2-13a4-40b0-8721-f9b633cb9fd9",
   "metadata": {},
   "source": [
    "### Function to get the S3 output location of model evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6c797-a2f4-4e06-8acb-4749da122934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "def get_output_jsonl(bucket, eval_job_response, model, task_type, dataset):\n",
    "    prefix = \"{}{}/{}/models/{}/taskTypes/{}/datasets/{}\".format(\"/\".join(eval_job_response[\"outputDataConfig\"][\"s3Uri\"].split('/')[3:]), eval_job_response[\"jobName\"], eval_job_response[\"jobArn\"].split(\"/\")[1], model, task_type, dataset)\n",
    "    print(bucket, prefix)\n",
    "    response = s3_client.list_objects(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix,\n",
    "    )\n",
    "    print(response)\n",
    "    return response['Contents'][0]['Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3169e-b677-429f-9886-36a9474e9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val1 = get_eval_job1['inferenceConfig']['models'][0]['bedrockModel']['modelIdentifier'].split('/')[-1]\n",
    "model_val2 = get_eval_job2['inferenceConfig']['models'][0]['bedrockModel']['modelIdentifier'].split('/')[-1]\n",
    "\n",
    "bucket_job1 = get_eval_job1[\"outputDataConfig\"][\"s3Uri\"].split('/')[2]\n",
    "job1_output = get_output_jsonl(bucket_job1, get_eval_job1, model_val1, task_type, dataset)\n",
    "print(job1_output)\n",
    "bucket_job2 = get_eval_job2[\"outputDataConfig\"][\"s3Uri\"].split('/')[2]\n",
    "job2_output = get_output_jsonl(bucket_job2, get_eval_job2, model_val2, task_type, dataset)\n",
    "print(job2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fb8e9-2f3e-4d57-ad62-dab3a5f65167",
   "metadata": {},
   "source": [
    "### Function to retrieve metrics from the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247286a-8650-4850-9ddc-e99b4aeaec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "s3_res = boto3.resource('s3')\n",
    "\n",
    "def retrieve_metrics(bucket, output_jsonl):\n",
    "    content_object = s3_res.Object(bucket, output_jsonl)\n",
    "    jsonl_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "    output_content = [json.loads(jline) for jline in jsonl_content.splitlines()]\n",
    "    return output_content\n",
    "\n",
    "job1_metrics =  retrieve_metrics(bucket_job1, job1_output)\n",
    "job2_metrics =  retrieve_metrics(bucket_job2, job2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8dd710-5c27-468b-9ef0-5a1acd5da8fc",
   "metadata": {},
   "source": [
    "### Function to filter and load the metrics in pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2e685-bf53-403d-9af9-19f2129086cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def pd_metrics(model1, model2, metric, job1_metrics, job2_metrics):\n",
    "    met1 = []\n",
    "    met2 = []\n",
    "    met_index = [job1_metrics[0]['automatedEvaluationResult']['scores'].index(i) for i in job1_metrics[0]['automatedEvaluationResult']['scores'] if i[\"metricName\"]==metric]\n",
    "    for i, (x, y) in enumerate(zip(job1_metrics, job2_metrics)):\n",
    "        met1.append(x['automatedEvaluationResult']['scores'][met_index[0]]['result'])\n",
    "        met2.append(y['automatedEvaluationResult']['scores'][met_index[0]]['result'])\n",
    "    met = pd.DataFrame({model1.split(':')[0]: met1, model2.split(':')[0]: met2})\n",
    "    return met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fadcf6-e126-4fc3-8c5f-7c9be0dd7a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = [m.split('.')[1] for m in metric_names]\n",
    "stats_list = []\n",
    "for metric in metric_names:\n",
    "    met_pd = pd_metrics(model_1, model_2, metric, job1_metrics, job2_metrics)\n",
    "    \n",
    "    stats_list.append(met_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708dc726-34ef-4815-835d-79a88ac9eb43",
   "metadata": {},
   "source": [
    "### Function to line plot for model comparison per metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f02877-af69-425c-ba6d-8324adec316b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_line_metrics(metrics, stats_list):\n",
    "    for metric, df in zip(metrics, stats_list):\n",
    "        print(\"\\n \\n \\n\")\n",
    "        if metric == \"Toxicity\":\n",
    "            sub = \"    Lower the better\"\n",
    "        else:\n",
    "            sub = \"    Higher the better\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.lineplot(data=df, markers=True, palette=\"flare\")\n",
    "        plt.legend(title='Model')\n",
    "        plt.xlabel('Inference test')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(metric)\n",
    "        plt.figtext(0.5, 0.01, sub, horizontalalignment='center', verticalalignment='bottom', fontsize=10, fontstyle='italic', color='purple')\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c234c2-1427-4560-b325-e80266a5360a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_line_metrics(metrics, stats_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ca750-042f-4bdf-ba46-efedaaab779a",
   "metadata": {},
   "source": [
    "### Function to plot bar chart for avg accuracy per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac336b-93f2-4fec-b784-e1c1f565f669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plt_acc_bar(df, metric):\n",
    "    # Calculate the average of each column\n",
    "    column_averages = df.mean()\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure()\n",
    "    sns.barplot(x=column_averages.index, y=column_averages.values)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(\"Average metric - {}\".format(metric))\n",
    "    plt.figtext(0.5, -0.01, \"   Higher the better\", horizontalalignment='center', verticalalignment='bottom', fontsize=10, fontstyle='italic', color='purple')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Average Value')\n",
    "\n",
    "    # Rotate x-axis labels if there are many columns\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Add value labels on top of each bar\n",
    "    for i, v in enumerate(column_averages.values):\n",
    "        plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d73b2-7090-441b-953c-267299215755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Average Accuracy\n",
    "plt_acc_bar(stats_list[0], metrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f68009-ff68-4a7a-bec0-94838a702572",
   "metadata": {},
   "source": [
    "### Function to bin the accuracy data in different accuracy(in percentage) bins [0, 20, 40, 60, 80, 100] and compare between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598fb861-35f2-438d-bb2c-0ef706401ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def bin_data(series, bins_list):\n",
    "    bins = pd.cut(series, bins=bins_list)\n",
    "    return bins, bins.value_counts().index\n",
    "\n",
    "def plot_bin_accuracy(df, bins_list):\n",
    "    # Apply binning to both columns\n",
    "    df_binned = df.apply(lambda x: bin_data(x, bins_list)[0])\n",
    "    bin_edges = bin_data(df.values.flatten(), bins_list)[1]\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    df_melted = df_binned.melt(var_name='model', value_name='bin')\n",
    "\n",
    "    # Count the occurrences of each bin for each model\n",
    "    df_counted = df_melted.groupby(['model', 'bin']).size().reset_index(name='count')\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='bin', y='count', hue='model', data=df_counted)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Comparison of Accuracy Range Across Two Models')\n",
    "    plt.figtext(0.5, -0.01, \"    Higher the better\", horizontalalignment='center', verticalalignment='bottom', fontsize=10, fontstyle='italic', color='purple')\n",
    "    plt.xlabel('Accuracy Range')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Model')\n",
    "\n",
    "    # Set x-axis labels to actual bin ranges\n",
    "    plt.xticks(range(len(bin_edges)), [f'({interval.left:.2f}, {interval.right:.2f}]' for interval in bin_edges], rotation=45, ha='right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd12bce-612a-42a9-9cdc-609af02f929f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bin_accuracy(stats_list[0], bins_list=[0, 0.2, 0.4, 0.6, 0.8, 1.0]) #update the bin values as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62763e41-ca05-4197-a8ec-0f40583e382a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <ins> Automatic Model Evaluation using  Custom Dataset </ins>\n",
    "\n",
    "Now lets start evaluating the same models with a custom dataset. \n",
    "\n",
    "*For this demo purpose only, we use Databricks Dolly-15k Dataset from HuggingFace.*\n",
    "\n",
    "**Note: Customers may use their own validation(groundtruth) dataset in the given format below based on their workload.**\n",
    "\n",
    "\n",
    "You can create a custom prompt dataset in an automatic model evaluation jobs. Custom prompt datasets must be stored in Amazon S3, and use the JSON line format and use the .jsonl file extension. Each line must be a valid JSON object. There can be up to 1000 prompts in your dataset per automatic evaluation job.\n",
    "\n",
    "**Custom dataset must use the following keys value pairs format.**\n",
    "\n",
    "`prompt` ‚Äì required to indicate the input for the following tasks:\n",
    "* The prompt that your model should respond to, in general text generation.\n",
    "* The question that your model should answer in the question and answer task type.\n",
    "* The text that your model should summarize in text summarization task.\n",
    "* The text that your model should classify in classification tasks.\n",
    "\n",
    "`referenceResponse` ‚Äì required to indicate the ground truth response against which your model is evaluated for the following tasks types:\n",
    "* The answer for all prompts in question and answer tasks.\n",
    "* The answer for all accuracy, and robustness evaluations.\n",
    "\n",
    "`category` ‚Äì (optional) generates evaluation scores reported for each category.\n",
    "\n",
    "As an example, accuracy requires both the question asked, and a answer to check the model's response against. In this example, use the key `prompt` with the value contains the question, the key `referenceResponse` with the value contains the answer and the key `category` contains the category of the question as follows.\n",
    "\n",
    "```\n",
    "{\"prompt\": \"Are The Smiths a good band?\", \n",
    "\"referenceResponse\": \"The Smiths were one of the most critically acclaimed bands to come from England in the 1980s. Typically classified as an \\\"indie rock\\\" band, the band released 4 albums from 1984 until their breakup in 1987. The band members, notably Morrissey and Johnny Marr, would go on to accomplish successful solo careers.\",\n",
    "\"category\": \"general_qa\"}\n",
    "```\n",
    "\n",
    "Please refer to https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets.html#model-evaluation-prompt-datasets-custom for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161f2f4-f13b-45d3-ad60-52e613ac27e3",
   "metadata": {},
   "source": [
    "### Download dolly-15k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64c53d-8c9d-4d04-980a-a302749e7537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl --no-check-certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a12f489-793c-4858-ae98-e5c9e5b0d652",
   "metadata": {},
   "source": [
    "### In this example, we will sample 100 records of \"open_qa\" category from dolly-15k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ad499-7204-46ec-9fb5-31744d180eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter and select 100 records from dolly dataset\n",
    "import json\n",
    "\n",
    "def filter_jsonl(data, key, value):\n",
    "    filtered_data = []\n",
    "    for item in data:\n",
    "        if item.get(key) == value:\n",
    "            filtered_data.append(item)\n",
    "    return filtered_data\n",
    "\n",
    "with open('databricks-dolly-15k.jsonl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "filtered_data = filter_jsonl(data, \"category\", \"open_qa\")[:100]\n",
    "print(len(filtered_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4d759-8c0e-4b32-b62d-1f258bb7c99a",
   "metadata": {},
   "source": [
    "### Function to modify the format as needed for custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b637d7-973b-453c-87cd-6cde34ac6df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_jsonl = './custom_dataset.jsonl'\n",
    "\n",
    "def write_jsonl(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in data:\n",
    "            item_mod = {}\n",
    "            item_mod['prompt'] = item['instruction']\n",
    "            item_mod['referenceResponse'] = item['response']\n",
    "            item_mod['category'] = item['category']\n",
    "            f.write(json.dumps(item_mod) + '\\n')\n",
    "\n",
    "# Write to JSONL file\n",
    "write_jsonl(filtered_data, custom_jsonl)\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d878f5d-9cc3-4ac6-922f-66846b459ae6",
   "metadata": {},
   "source": [
    "### Upload dataset jsonl to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad13d6-2672-4d42-a6b6-6756091f544d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_res = boto3.resource('s3')\n",
    "s3_res.Bucket(bucket).upload_file(custom_jsonl, 'custom_datasets/dolly/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f59d1-842b-4075-945c-c873206abec8",
   "metadata": {},
   "source": [
    "### Choose task_type, metrics and s3 input/output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752ad95-9c13-43cc-b99a-dd6d7c8818ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_type = \"QuestionAndAnswer\"\n",
    "metric_names = [\"Builtin.Accuracy\", \"Builtin.Robustness\", \"Builtin.Toxicity\"] #Add or remove metrics within the list format\n",
    "output_path = \"s3://{}/outputs/\".format(bucket)\n",
    "cus_ds_s3 = \"s3://{}/custom_datasets/dolly/\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18dd74-982d-4334-aee2-dc066adf20dc",
   "metadata": {},
   "source": [
    "### Submit automatic model evaluation jobs with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc25d7-2195-4963-9313-fed6aa8743a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "cust_eval_jobs = []\n",
    "for model_arn in model_arns:\n",
    "    job_name = \"eval-custom-{}-{}\".format(model_arn.split('/')[-1].split(':')[0], str(datetime.datetime.now().timestamp()).split('.')[0])\n",
    "    job_name = job_name.replace(\".\", \"-\")\n",
    "    print(job_name)\n",
    "    job = model_eval(model_arn, \"dolly-open-qa-custom\", task_type, output_path, job_name, metric_names, custom_ds=True, custom_ds_s3=cus_ds_s3)\n",
    "    cust_eval_jobs.append(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65039c01-4ab2-41aa-9bd8-1f5d863ca59e",
   "metadata": {},
   "source": [
    "### Track evaluation job status until \"COMPLETED\" or \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177886eb-1f18-4872-b8b9-b148e02aa317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_cust_eval_job1, get_cust_eval_job2 = check_job_status(cust_eval_jobs,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f7b8d-ea59-4d4c-b96c-e4a642a08eff",
   "metadata": {},
   "source": [
    "### Get evaluation jobs output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec5474-7ad9-4696-828f-a0630be60a8c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; border-left: 4px solid #28a745; padding: 15px; border-radius: 5px;\">\n",
    "\n",
    "<strong>The evaluation jobs you just submitted may take several minutes to complete.</strong><br><br>\n",
    "\n",
    "\n",
    "Instead of waiting for the submitted evaluation job(s) to complete, let's proceed with monitoring and analyzing results from previously completed jobs. This approach allows us to:\n",
    "\n",
    "‚è±Ô∏è Make productive use of our workshop time.\n",
    "\n",
    "üß† Understand the evaluation framework and metrics.\n",
    "\n",
    "üìà Compare existing model performance results.\n",
    "\n",
    "In the following cells, we'll:\n",
    "\n",
    "üîÑ Check the status of our submitted job(s).\n",
    "\n",
    "üì• Retrieve and analyze results from completed evaluation jobs.\n",
    "\n",
    "‚öñÔ∏è Compare performance across different models.\n",
    "\n",
    "üìä Visualize key metrics and insights.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e8bd7-c568-4d7d-98fc-793616233d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_jobs = get_completed_automatic_jobs(custom=True)\n",
    "\n",
    "if len(completed_jobs) >= 2:\n",
    "    get_eval_job1 = bedrock_client.get_evaluation_job(jobIdentifier=completed_jobs[0]['jobArn'])\n",
    "    print(f\"Job1 name: {get_eval_job1['jobName']}\")\n",
    "    get_eval_job2 = bedrock_client.get_evaluation_job(jobIdentifier=completed_jobs[1]['jobArn'])\n",
    "    print(f\"Job2 name: {get_eval_job2['jobName']}\")\n",
    "else:\n",
    "    print(f\"Only found {len(completed_jobs)} completed jobs. Need to wait for jobs to complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f6524d-9d9b-4370-94f3-5e19e7df17d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_val1 = get_eval_job1['inferenceConfig']['models'][0]['bedrockModel']['modelIdentifier'].split('/')[-1]\n",
    "model_val2 = get_eval_job2['inferenceConfig']['models'][0]['bedrockModel']['modelIdentifier'].split('/')[-1]\n",
    "\n",
    "bucket_cust_job1 = get_eval_job1[\"outputDataConfig\"][\"s3Uri\"].split('/')[2]\n",
    "print(bucket_cust_job1)\n",
    "cust_job1_output = get_output_jsonl(bucket_cust_job1, get_eval_job1, model_val1, task_type, dataset=\"dolly-open-qa-custom\")\n",
    "print(cust_job1_output)\n",
    "bucket_cust_job2 = get_eval_job2[\"outputDataConfig\"][\"s3Uri\"].split('/')[2]\n",
    "cust_job2_output = get_output_jsonl(bucket_cust_job2, get_eval_job2, model_val2, task_type, dataset=\"dolly-open-qa-custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3dd0b0-5388-4244-9a93-ed105f30887d",
   "metadata": {},
   "source": [
    "### Retrieve metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1aaf83-d050-422f-bb36-c65e602d8693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cust_job1_metrics =  retrieve_metrics(bucket_cust_job1, cust_job1_output)\n",
    "cust_job2_metrics =  retrieve_metrics(bucket_cust_job2, cust_job2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b4712-9ab7-40b9-8353-d32c5131fec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = [m.split('.')[1] for m in metric_names]\n",
    "stats_list = []\n",
    "for metric in metric_names:\n",
    "    met_pd = pd_metrics(model_1, model_2, metric, job1_metrics, job2_metrics)\n",
    "    \n",
    "    stats_list.append(met_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68661fc9-d34e-47ad-bd6c-f7e81eed7e9c",
   "metadata": {},
   "source": [
    "### Draw line plot for model comparison per metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b89ca-c7d2-4532-aced-370f52fac732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_line_metrics(metrics, stats_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565eeaf-44b7-4cf0-a294-c2218e70a592",
   "metadata": {},
   "source": [
    "### Average Accuracy per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fba552-a916-498b-b299-7b31e6a7a0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt_acc_bar(stats_list[0], metrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32df504-726d-4aee-a477-b502da59f8ee",
   "metadata": {},
   "source": [
    "### Plot across different ranges of accuracy and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271cda0-b881-42fb-a02f-0ed268d41103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bin_accuracy(stats_list[0], bins_list=[0, 0.2, 0.4, 0.6, 0.8, 1.0]) #update the bin values as needed"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
