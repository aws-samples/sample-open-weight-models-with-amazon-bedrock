{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddfd6a4-6e2f-4eae-aa70-34fa0a80452b",
   "metadata": {},
   "source": [
    "# Amazon Bedrock LLM-as-a-Judge Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7baebae-3a3b-435d-bff4-7f111a97ea7c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use Amazon Bedrock's Model-as-a-Judge feature for systematic model evaluation. The Model-as-a-Judge approach uses a foundation model to score another model's responses and provide explanations for the scores. The guide covers creating evaluation datasets, running evaluations, and comparing different foundation models.\n",
    "\n",
    "Please refer to [official documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-judge.html) for more details including supported evaluator and generator models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecd017a-ee09-45bc-ab07-06e7059d9e07",
   "metadata": {},
   "source": [
    "### The Role of Evaluator and Generator Models in Amazon Bedrock LLM-as-Judge Evaluation\n",
    "In Amazon Bedrock's LLM-as-Judge evaluation framework, two distinct model roles work together to enable robust assessment of language model outputs:\n",
    "\n",
    "**Generator Models**\n",
    "\n",
    "Generator models are the models being evaluated. They:\n",
    "\n",
    "* Produce responses to the input prompts in your evaluation dataset\n",
    "* Represent the candidates whose performance you want to assess\n",
    "* Can be different foundation models (e.g., Llama, Qwen, DeepSeek, Nova, etc.,) or the same model with different fine-tuning or prompt engineering approaches\n",
    "* Are the subjects of comparison in A/B testing scenarios\n",
    "* Generate outputs that will be scored by the evaluator model\n",
    "\n",
    "**Evaluator Models (LLM-as-Judge)**\n",
    "\n",
    "Evaluator models serve as automated judges that:\n",
    "\n",
    "* Assess the quality of responses from generator models\n",
    "* Apply scoring criteria defined in your evaluation job configuration\n",
    "* Provide numerical ratings and explanatory feedback for each response\n",
    "* Act as impartial judges to compare multiple model responses objectively\n",
    "* Replace or supplement human evaluation, offering scalable assessment\n",
    "* Should ideally be powerful models with strong reasoning capabilities \n",
    "\n",
    "**How They Interact in the Evaluation Process**\n",
    "* You define evaluation prompts and metrics in your evaluation dataset\n",
    "* Generator models produce responses to these prompts\n",
    "* The evaluator model reviews each response according to specified criteria\n",
    "* The evaluator provides scores and justifications for each assessment\n",
    "* Bedrock aggregates these results into comprehensive evaluation reports\n",
    "\n",
    "This automated approach enables systematic comparison of model outputs across various dimensions like accuracy, helpfulness, relevance, and safety, while reducing the need for extensive human evaluation.\n",
    "\n",
    "The separation of generator and evaluator roles allows for fair, consistent assessment across different model types and configurations, helping you identify the best-performing models for your specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98a82b-f609-4ef9-b727-6383e618fb33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T06:50:09.482434Z",
     "iopub.status.busy": "2025-10-29T06:50:09.482156Z",
     "iopub.status.idle": "2025-10-29T06:50:09.489484Z",
     "shell.execute_reply": "2025-10-29T06:50:09.488447Z",
     "shell.execute_reply.started": "2025-10-29T06:50:09.482411Z"
    }
   },
   "source": [
    "### Use case: \n",
    "In this lab, you will **evaluate LLM performance on mathematical computations in the context of shopping**. You will use Mistral Large as the evaluator model, and Meta Llama 3.1 8B and Mistral 7B Instruct as the generator models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bc0ff-3511-4778-96b9-6fff58278318",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. An AWS account with Bedrock access\n",
    "2. Appropriate IAM roles and permissions\n",
    "3. An S3 bucket for storing evaluation data\n",
    "\n",
    "Let's begin with updating boto3 to latest version and install other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996e458-cf78-44fa-8dee-82427d7d3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8003ff1-2d64-46e6-8655-c1a2c6c610d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client('bedrock')\n",
    "found_models = [m['modelId'] for m in bedrock_client.list_foundation_models(byOutputModality='TEXT')['modelSummaries']]\n",
    "eval_model = [fm for fm in found_models if \"mistral.mistral-large-2402\" in fm][0]\n",
    "print(f\"Evaluator Model: {eval_model}\")\n",
    "gen_models = [ \"meta.llama3-1-8b-instruct-v1:0\", \"mistral.mistral-7b-instruct-v0:2\"]\n",
    "print(f\"Generator Models: {gen_models}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859381b4-7192-4291-84c9-28fd6aac7a64",
   "metadata": {},
   "source": [
    "### Choose a S3 Bucket for Model Evaluation jobs\n",
    "----\n",
    "\n",
    "Bedrock model evaluation jobs require an Amazon S3 bucket in your current AWS region to store input datasets and model evaluation results.\n",
    "\n",
    "\n",
    "**If you're running this notebook in the JupyterLab environment in Amazon SageMaker AI Studio, you can use your the default bucket from the Amazon SageMaker session to store the datasets and evaluation results. To do so, run the code below as-is.** \n",
    "\n",
    "For users running this notebook outside SageMaker AI Studio (for example on a local machine or EC2 instance), you'll need to either create a new S3 bucket or specify an existing one in your region. Please follow the instructions within the cell before you execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9d082-1adc-46bc-8698-0f2844061856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment out the following 2 lines if not running on SageMaker AI Studio notebooks\n",
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "#If you want to use a custom s3 bucket or running this notebook outside of SageMaker AI Studio, please mention the bucket name as follows\n",
    "#bucket = \"\"\n",
    "bucket=None\n",
    "\n",
    "if bucket is None and sess is not None: \n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket) #comment out if Sagemaker is not used\n",
    " \n",
    "\n",
    "print(f\"Model Evaluation bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa833af-e7ae-489a-a9d4-c36a3533f7d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T06:41:36.419091Z",
     "iopub.status.busy": "2025-10-29T06:41:36.418800Z",
     "iopub.status.idle": "2025-10-29T06:41:36.428158Z",
     "shell.execute_reply": "2025-10-29T06:41:36.427146Z",
     "shell.execute_reply.started": "2025-10-29T06:41:36.419070Z"
    }
   },
   "source": [
    "Next, you need to allow the IAM service role to access to the S3 bucket you specified. For that, you define the IAM policy and assign it to the role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefa452-e9fa-4227-bdf3-4b87cd28e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore role_arn and role_name created in lab2a to run eval jobs\n",
    "%store -r role_arn\n",
    "%store -r role_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d71c2-2c9e-4141-aa3b-52d11576fa88",
   "metadata": {},
   "source": [
    "## Run an Amazon Bedrock LLM-as-a-Judge Evaluation job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acd756-2e72-42c7-a1c2-9b149dca2c9f",
   "metadata": {},
   "source": [
    "### Generate the dataset\n",
    "You'll create a simple dataset of mathematical reasoning problems. These problems test:\n",
    "\n",
    "1. Basic arithmetic\n",
    "2. Logical reasoning\n",
    "3. Natural language understanding\n",
    "\n",
    "The dataset follows the required JSONL format for Bedrock evaluation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbc311-1bae-43e6-b78f-2b104c141f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def generate_shopping_problems(num_problems=50):\n",
    "    \"\"\"Generate shopping-related math problems with random values.\"\"\"\n",
    "    problems = []\n",
    "    items = [\"apples\", \"oranges\", \"bananas\", \"books\", \"pencils\", \"notebooks\"]\n",
    "    \n",
    "    for _ in range(num_problems):\n",
    "        # Generate random values\n",
    "        item = random.choice(items)\n",
    "        quantity = random.randint(3, 20)\n",
    "        price_per_item = round(random.uniform(1.5, 15.0), 2)\n",
    "        discount_percent = random.choice([10, 15, 20, 25, 30])\n",
    "        \n",
    "        # Calculate the answer\n",
    "        total_price = quantity * price_per_item\n",
    "        discount_amount = total_price * (discount_percent / 100)\n",
    "        final_price = round(total_price - discount_amount, 2)\n",
    "        \n",
    "        # Create the problem\n",
    "        problem = {\n",
    "            \"prompt\": f\"If {item} cost \\${price_per_item} each and you buy {quantity} of them with a {discount_percent}% discount, how much will you pay in total?\",\n",
    "            \"category\": \"Shopping Math\",\n",
    "            \"referenceResponse\": f\"The total price will be \\${final_price}. Original price: \\${total_price} minus {discount_percent}% discount (\\${discount_amount})\"\n",
    "        }\n",
    "        \n",
    "        problems.append(problem)\n",
    "    \n",
    "    return problems\n",
    "\n",
    "def save_to_jsonl(problems, output_file):\n",
    "    \"\"\"Save the problems to a JSONL file.\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for problem in problems:\n",
    "            f.write(json.dumps(problem) + '\\n')\n",
    "\n",
    "SAMPLE_SIZE = 30\n",
    "dataset_custom_name = \"eval_dataset\"\n",
    "problems = generate_shopping_problems(SAMPLE_SIZE)\n",
    "save_to_jsonl(problems, f\"{dataset_custom_name}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7efcc11-4ebc-4d5d-bd82-16791cbe3a6a",
   "metadata": {},
   "source": [
    "After generating the sample dataset, you need to upload it to S3 for use in the evaluation job. You'll use the boto3 S3 client to upload our JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593969c8-443a-4747-b35c-00639f6d9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(local_file: str, bucket: str, s3_key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Upload a file to S3 with error handling.\n",
    "    \n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.upload_file(local_file, bucket, s3_key)\n",
    "        print(f\"‚úì Successfully uploaded to s3://{bucket}/{s3_key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error uploading to S3: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Upload dataset\n",
    "PREFIX = \"bedrock_model_eval\"\n",
    "s3_key = f\"{PREFIX}/{dataset_custom_name}.jsonl\"\n",
    "upload_success = upload_to_s3(f\"{dataset_custom_name}.jsonl\", bucket, s3_key)\n",
    "\n",
    "if not upload_success:\n",
    "    raise Exception(\"Failed to upload dataset to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea508f3-e1a5-490b-8b20-295e1348bdd8",
   "metadata": {},
   "source": [
    "### Configure the evaluation jobs\n",
    "\n",
    "You are now ready to configure the LLM-as-Judge evaluation jobs. With Amazon Bedrock LLM-as-a-Judge evaluation you can use comprehensive metrics to assess model performance:\n",
    "\n",
    "| Metric    | Description |\n",
    "| -------- | ------- |\n",
    "| Quality  | Correctness, Completeness, Faithfulness    |\n",
    "| User Experience | Helpfulness, Coherence, Relevance     |\n",
    "| Instructions    | Following Instructions, Professional Style    |\n",
    "| Safety    | Harmfulness, Stereotyping, Refusal    |\n",
    "\n",
    "The following code configures the jobs using the boto3 SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed635d7-d33e-4d21-bf4a-c2fa13bc8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_judge_evaluation(\n",
    "    client,\n",
    "    job_name: str,\n",
    "    role_arn: str,\n",
    "    input_s3_uri: str,\n",
    "    output_s3_uri: str,\n",
    "    evaluator_model_id: str,\n",
    "    generator_model_id: str,\n",
    "    dataset_name: str = None,\n",
    "    task_type: str = \"General\" # must be General for LLMaaJ\n",
    "):    \n",
    "    # All available LLM-as-judge metrics\n",
    "    llm_judge_metrics = [\n",
    "        \"Builtin.Correctness\",\n",
    "        \"Builtin.Completeness\", \n",
    "        \"Builtin.Faithfulness\",\n",
    "        \"Builtin.Helpfulness\",\n",
    "        \"Builtin.Coherence\",\n",
    "        \"Builtin.Relevance\",\n",
    "        \"Builtin.FollowingInstructions\",\n",
    "        \"Builtin.ProfessionalStyleAndTone\",\n",
    "        \"Builtin.Harmfulness\",\n",
    "        \"Builtin.Stereotyping\",\n",
    "        \"Builtin.Refusal\"\n",
    "    ]\n",
    "\n",
    "    # Configure dataset\n",
    "    dataset_config = {\n",
    "        \"name\": dataset_name or \"CustomDataset\",\n",
    "        \"datasetLocation\": {\n",
    "            \"s3Uri\": input_s3_uri\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.create_evaluation_job(\n",
    "            jobName=job_name,\n",
    "            roleArn=role_arn,\n",
    "            applicationType=\"ModelEvaluation\",\n",
    "            evaluationConfig={\n",
    "                \"automated\": {\n",
    "                    \"datasetMetricConfigs\": [\n",
    "                        {\n",
    "                            \"taskType\": task_type,\n",
    "                            \"dataset\": dataset_config,\n",
    "                            \"metricNames\": llm_judge_metrics\n",
    "                        }\n",
    "                    ],\n",
    "                    \"evaluatorModelConfig\": {\n",
    "                        \"bedrockEvaluatorModels\": [\n",
    "                            {\n",
    "                                \"modelIdentifier\": evaluator_model_id\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            inferenceConfig={\n",
    "                \"models\": [\n",
    "                    {\n",
    "                        \"bedrockModel\": {\n",
    "                            \"modelIdentifier\": generator_model_id\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            outputDataConfig={\n",
    "                \"s3Uri\": output_s3_uri\n",
    "            }\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating evaluation job: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edb051-a16c-4dce-9f11-6b4b05c716ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T07:00:02.077264Z",
     "iopub.status.busy": "2025-10-29T07:00:02.076972Z",
     "iopub.status.idle": "2025-10-29T07:00:02.080496Z",
     "shell.execute_reply": "2025-10-29T07:00:02.079614Z",
     "shell.execute_reply.started": "2025-10-29T07:00:02.077238Z"
    }
   },
   "source": [
    "### Run evaluation jobs for the 2 generator models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1ed5f-f7fc-43b9-916a-3b108c504917",
   "metadata": {},
   "source": [
    "Next, trigger the evaluation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5336bb3-ae01-447a-a2c5-92bd5b73d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{bucket}/model_eval_output\"\n",
    "task_type=\"General\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f717b59-ebc8-498f-b695-5ca4e0277598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def run_model_comparison(\n",
    "    generator_models: List[str],\n",
    "    evaluator_model: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    evaluation_jobs = []\n",
    "    \n",
    "    for generator_model in generator_models:\n",
    "        job_name = f\"{generator_model.split('.')[1].split(':')[0]}-{evaluator_model.split('.')[0]}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "        \n",
    "        try:\n",
    "            response = create_llm_judge_evaluation(\n",
    "                client=bedrock_client,\n",
    "                job_name=job_name,\n",
    "                role_arn=role_arn,\n",
    "                input_s3_uri=f\"s3://{bucket}/{PREFIX}/{dataset_custom_name}.jsonl\",\n",
    "                output_s3_uri=f\"s3://{output_path}/{job_name}/\",\n",
    "                evaluator_model_id=evaluator_model,\n",
    "                generator_model_id=generator_model,\n",
    "                task_type=task_type\n",
    "            )\n",
    "            \n",
    "            job_info = {\n",
    "                \"job_name\": job_name,\n",
    "                \"job_arn\": response[\"jobArn\"],\n",
    "                \"generator_model\": generator_model,\n",
    "                \"evaluator_model\": evaluator_model,\n",
    "                \"status\": \"CREATED\"\n",
    "            }\n",
    "            evaluation_jobs.append(job_info)\n",
    "            \n",
    "            print(f\"‚úì Created job: {job_name}\")\n",
    "            print(f\"  Generator: {generator_model}\")\n",
    "            print(f\"  Evaluator: {evaluator_model}\")\n",
    "            print(\"-\" * 80)\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error with {generator_model}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return evaluation_jobs\n",
    "\n",
    "# Run model comparison\n",
    "evaluation_jobs = run_model_comparison(gen_models, eval_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a871821-65c8-4394-9cbb-e389fee317c5",
   "metadata": {},
   "source": [
    "### Monitoring and Results\n",
    "The jobs will take several minutes to complete. You can monitor the progress of the evaluation jobs and display their current status before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62c5c3-2e60-4c3b-bb31-738229d0f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime \n",
    "# function to check job status\n",
    "def check_jobs_status(jobs, client):\n",
    "    \"\"\"Check and update status for all evaluation jobs\"\"\"\n",
    "    for job in jobs:\n",
    "        try:\n",
    "            response = client.get_evaluation_job(\n",
    "                jobIdentifier=job[\"job_arn\"]\n",
    "            )\n",
    "            job[\"status\"] = response[\"status\"]\n",
    "        except Exception as e:\n",
    "            job[\"status\"] = f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    return jobs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02738354-1fc2-4132-92d4-41f21019e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def check_status(evaluation_jobs, loop=True):\n",
    "    max_time = time.time() + 2*60*60 \n",
    "    \n",
    "    while True:\n",
    "        now = datetime.datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        updated_jobs = check_jobs_status(evaluation_jobs, bedrock_client)\n",
    "        \n",
    "        job1_status, job2_status = updated_jobs[0][\"status\"], updated_jobs[1][\"status\"]\n",
    "        \n",
    "        if loop:\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "        print(f\"{current_time} : Model evaluation job1 is {job1_status} and job2 is {job2_status}.\")\n",
    "        \n",
    "        if not loop or (job1_status == \"Completed\" or job1_status == \"Failed\") and (job2_status == \"Completed\" or job2_status == \"Failed\") or time.time() >= max_time:\n",
    "            break\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    return job1_status, job2_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8b11d-af5c-4989-b5dd-50a6b587f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "status1, status2 = check_status(evaluation_jobs, loop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce723e-7cde-4422-ae2c-548f35435ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "display(Markdown(f\"You can also review the status of the jobs in the [Amazon Bedrock Console](https://{region}.console.aws.amazon.com/bedrock/home?region={region}#/eval/evaluation)\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e6fbdf-1566-49df-935e-97096d9e083f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T21:55:03.503969Z",
     "iopub.status.busy": "2025-11-09T21:55:03.503590Z",
     "iopub.status.idle": "2025-11-09T21:55:03.514715Z",
     "shell.execute_reply": "2025-11-09T21:55:03.513676Z",
     "shell.execute_reply.started": "2025-11-09T21:55:03.503936Z"
    }
   },
   "source": [
    "<div style=\"background-color: #d4edda; border-left: 4px solid #28a745; padding: 15px; border-radius: 5px;\">\n",
    "\n",
    "<strong>The evaluation jobs you just submitted may take several minutes to complete.</strong><br><br>\n",
    "\n",
    "\n",
    "Instead of waiting for the submitted evaluation job(s) to complete, let's proceed with monitoring and analyzing results from previously completed jobs. This approach allows us to:\n",
    "\n",
    "‚è±Ô∏è Make productive use of our workshop time.\n",
    "\n",
    "üß† Understand the evaluation framework and metrics.\n",
    "\n",
    "üìà Compare existing model performance results.\n",
    "\n",
    "In the following cells, we'll:\n",
    "\n",
    "üîÑ Check the status of our submitted job(s).\n",
    "\n",
    "üì• Retrieve and analyze results from completed evaluation jobs.\n",
    "\n",
    "‚öñÔ∏è Compare performance across different models.\n",
    "\n",
    "üìä Visualize key metrics and insights.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292f920-d15d-43f6-a3c2-1e4b7b4e1de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T22:10:04.753999Z",
     "iopub.status.busy": "2025-11-09T22:10:04.753710Z",
     "iopub.status.idle": "2025-11-09T22:10:04.760409Z",
     "shell.execute_reply": "2025-11-09T22:10:04.759499Z",
     "shell.execute_reply.started": "2025-11-09T22:10:04.753978Z"
    }
   },
   "source": [
    "Next, you retrieve the most recent jobs run for the generator, evaluator and task type combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71856dc-56c6-4b32-8fb9-17d6854f468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "bedrock = boto3.client('bedrock', region_name=region)\n",
    "\n",
    "def get_completed_llm_judge_jobs(hours_ago=1):\n",
    "    all_jobs = []\n",
    "    next_token = None\n",
    "    \n",
    "    # Get all jobs with pagination\n",
    "    while True:\n",
    "        params = {\n",
    "            'sortBy': 'CreationTime',\n",
    "            'sortOrder': 'Descending',\n",
    "            'statusEquals': 'Completed',\n",
    "            'applicationTypeEquals': 'ModelEvaluation',\n",
    "            'maxResults': 1000\n",
    "        }\n",
    "        \n",
    "        if next_token:\n",
    "            params['nextToken'] = next_token\n",
    "            \n",
    "        response = bedrock.list_evaluation_jobs(**params)\n",
    "        all_jobs.extend(response['jobSummaries'])\n",
    "        \n",
    "        next_token = response.get('nextToken')\n",
    "        if not next_token:\n",
    "            break\n",
    "\n",
    "    # Filter jobs for LLM-as-judge evaluation\n",
    "    jobs = [\n",
    "        job for job in all_jobs \n",
    "        if 'evaluatorModelIdentifiers' in job\n",
    "        and any(job.get('modelIdentifiers', []) == [model] for model in gen_models)\n",
    "        and job.get('evaluatorModelIdentifiers', []) == [eval_model]\n",
    "    ]\n",
    "\n",
    "    # Group jobs by unique combination of generator model and evaluator model\n",
    "    job_groups = {}\n",
    "    \n",
    "    for job in jobs:\n",
    "        generator_model = job['modelIdentifiers'][0]\n",
    "        evaluator_model = job['evaluatorModelIdentifiers'][0]\n",
    "        key = (generator_model, evaluator_model)\n",
    "        \n",
    "        # Keep only the most recent job for each unique combination\n",
    "        if key not in job_groups or job['creationTime'] > job_groups[key]['creationTime']:\n",
    "            job_groups[key] = job\n",
    "    \n",
    "    return list(job_groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acdb5c6-2181-4209-8c69-66c5cec79217",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_jobs = get_completed_llm_judge_jobs()[:2]\n",
    "evaluation_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6a263-b4c5-44f8-8bbd-d78a885f3d49",
   "metadata": {},
   "source": [
    "## Review evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc78d61-448e-4db2-9c41-9d563af7cd6d",
   "metadata": {},
   "source": [
    "Next, retrieve the S3 output locations for each evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c0338-55d3-429f-ba79-6fd173a8cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "outputs_jsonl = []\n",
    "\n",
    "for job in evaluation_jobs:\n",
    "    job_details = bedrock.get_evaluation_job(jobIdentifier=job['jobArn'])\n",
    "    s3_uri = job_details['outputDataConfig']['s3Uri']\n",
    "    \n",
    "    # Parse S3 URI\n",
    "    bucket = s3_uri.split('/')[2]\n",
    "    prefix = '/'.join(s3_uri.split('/')[3:])\n",
    "    \n",
    "    # List objects\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    \n",
    "    jsonl_files = [f\"{obj['Key']}\" for obj in response.get('Contents', []) if obj['Key'].endswith('.jsonl')]\n",
    "    outputs_jsonl.extend(jsonl_files)\n",
    "outputs_jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482918c-3f39-4ae4-bffe-8157b23203a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T22:11:04.669250Z",
     "iopub.status.busy": "2025-11-09T22:11:04.668931Z",
     "iopub.status.idle": "2025-11-09T22:11:04.675655Z",
     "shell.execute_reply": "2025-11-09T22:11:04.674714Z",
     "shell.execute_reply.started": "2025-11-09T22:11:04.669224Z"
    }
   },
   "source": [
    "and the metrics calculated during the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78850f-244a-44ce-92c4-c916e4263f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve metrics from the output\n",
    "import json\n",
    "s3_res = boto3.resource('s3')\n",
    "\n",
    "def retrieve_metrics(bucket, output_jsonl):\n",
    "    content_object = s3_res.Object(bucket, output_jsonl)\n",
    "    jsonl_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "    output_content = [json.loads(jline) for jline in jsonl_content.splitlines()]\n",
    "    return output_content\n",
    "    \n",
    "eval_jobs_metrics_jsonl = [retrieve_metrics(bucket, output_jsonl) for output_jsonl in outputs_jsonl]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb5a342-aa53-457e-9743-e27e16c8d631",
   "metadata": {},
   "source": [
    "### Plot Metrics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c491fb-7ff6-4dd0-b018-5fe92d11ea40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:13:47.564600Z",
     "iopub.status.busy": "2025-10-29T09:13:47.564320Z",
     "iopub.status.idle": "2025-10-29T09:13:47.570890Z",
     "shell.execute_reply": "2025-10-29T09:13:47.569932Z",
     "shell.execute_reply.started": "2025-10-29T09:13:47.564578Z"
    }
   },
   "source": [
    "You can now visualize and compare model performance through detailed metric analysis. It processes evaluation results across 11 key metrics and generates plots. The visualization helps identify which model excels in specific areas like accuracy and coherence, making it easier to make data-driven decisions about model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41be2f-50d4-403f-b916-7cee0efb0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\n",
    "        \"Builtin.Correctness\",\n",
    "        \"Builtin.Completeness\", \n",
    "        \"Builtin.Faithfulness\",\n",
    "        \"Builtin.Helpfulness\",\n",
    "        \"Builtin.Coherence\",\n",
    "        \"Builtin.Relevance\",\n",
    "        \"Builtin.FollowingInstructions\",\n",
    "        \"Builtin.ProfessionalStyleAndTone\",\n",
    "        \"Builtin.Harmfulness\",\n",
    "        \"Builtin.Stereotyping\",\n",
    "        \"Builtin.Refusal\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5d77d-5aed-4f21-bcf3-44d149579d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to filter and load the metrics in pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "def pd_metrics(model1, model2, metric, job1_metrics, job2_metrics):\n",
    "    met1 = []\n",
    "    met2 = []\n",
    "    met_index = [job1_metrics[0]['automatedEvaluationResult']['scores'].index(i) for i in job1_metrics[0]['automatedEvaluationResult']['scores'] if i[\"metricName\"]==metric]\n",
    "    for i, (x, y) in enumerate(zip(job1_metrics, job2_metrics)):\n",
    "        met1.append(x['automatedEvaluationResult']['scores'][met_index[0]]['result'])\n",
    "        met2.append(y['automatedEvaluationResult']['scores'][met_index[0]]['result'])\n",
    "    met = pd.DataFrame({model1.split(':')[0]: met1, model2.split(':')[0]: met2})\n",
    "    return met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b719c1-5306-4c4c-a2f4-5cbd59ed3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stats_list = []\n",
    "for metric in metric_names:\n",
    "    met_pd = pd_metrics(gen_models[0], gen_models[1], metric, eval_jobs_metrics_jsonl[0], eval_jobs_metrics_jsonl[1])\n",
    "    stats_list.append(met_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45596c3-05e6-4674-8a2d-dc6f81a7a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to line plot for model comparison per metric\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "metrics = [m.split('.')[1] for m in metric_names]\n",
    "def plot_line_metrics(metrics, stats_list):\n",
    "    for metric, df in zip(metrics, stats_list):\n",
    "        print(\"\\n \\n \\n\")\n",
    "        ltb = [\"Refusal\", \"Sterotyping\", \"Harmfulness\"]\n",
    "        if metric in ltb:\n",
    "            sub = \"    Lower the better\"\n",
    "        else:\n",
    "            sub = \"    Higher the better\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.lineplot(data=df, markers=True, palette=\"flare\")\n",
    "        plt.legend(title='Model')\n",
    "        plt.xlabel('Inference test')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(metric)\n",
    "        plt.figtext(0.5, 0.01, sub, horizontalalignment='center', verticalalignment='bottom', fontsize=10, fontstyle='italic', color='purple')\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865a153-c709-4a80-b211-dc99d5daf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_metrics(metrics, stats_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
